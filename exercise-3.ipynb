{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"9G28EiwFNdBG","executionInfo":{"status":"ok","timestamp":1718208735875,"user_tz":-120,"elapsed":422,"user":{"displayName":"Parul Negi","userId":"14413694775804507286"}}},"outputs":[],"source":["from os.path import join as ospj\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22456,"status":"ok","timestamp":1718208759602,"user":{"displayName":"Parul Negi","userId":"14413694775804507286"},"user_tz":-120},"id":"C0Wp4rhNNdBH","outputId":"25e81b67-11d8-418e-95a1-ee2bfa7fbcbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["### If using Colab, uncomment the two following lines to mount your Google Drive.\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","### If using Colab, change the PROJECT_ROOT to where you've uploaded the project.\n","### E.g. PROJECT_ROOT='/content/drive/MyDrive/TeamX/'\n","### You may also need to change the `data_dir`, `save_dir`, paths in the `cfgs/exercise_3/` configs.\n","\n","PROJECT_ROOT='/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3'\n","import sys\n","sys.path.append(PROJECT_ROOT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-B4HPvLuNdBJ"},"outputs":[],"source":["# Just so that you don't have to restart the notebook with every change.\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"nO23A_d-c4t8"},"source":["In Exercise 3, you will implement a convolutional neural network to perform image classification and explore methods to improve the training performance and generalization of these networks.\n","We will use the CIFAR-10 dataset as a benchmark for our networks, similar to the previous exercise. This dataset consists of 50000 training images of 32x32 resolution with 10 object classes, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The task is to implement a convolutional network to classify these images using the PyTorch library. The four questions are,\n","\n","- Implementing a convolutional neural network, training it, and visualizing its weights (Question 1).\n","- Experiment with batch normalization and early stopping (Question 2).\n","- Data augmentation and dropout to improve generalization (Question 3).\n","- Implement transfer learning from an ImageNet-pretrained model (Question 4)."]},{"cell_type":"markdown","metadata":{"id":"SPPGGzQ3NdBL"},"source":["Before we begin, here are a few remarks regarding the codebase for this assignment.\n","\n","\n","For every experiment, you would define a config dictionary (see the dictionary in `./cfgs/exercise_3/cnn_cifar10.py`). Every config dictionary, will have the configuration for\n","- data (e.g batch size, shuffle, which DataModule to use, splitting)\n","- model (e.g which class module to use and what arguments to pass to it)\n","- training (e.g type of optimizer, lr_scheduler, n_epochs etc.)\n","\n","The DataModules are located at  `src/data_loaders/` and they inherit from a base_data_module that handles things such as splitting the data (see `src/data_loaders/base_data_modules.py`). A sample datamodule may inherit from this class (e.g `src/data_loaders/data_modules.py`). The main concern is that datamodule initialization should get everything ready, so that one can simply get the dataloaders for train/held-out sets from it (see `get_loader` and `get_heldout_loader` in BaseDataModule). The data augmentations are also done in a preset fation. One defines the preset in `utils/transform_presets.py` and simply specifies the *preset key* in the config for datamodule.\n","\n","The models are defined in `src/models/` (see for instance `src/models/cnn/model.py`). These are typical Pytorch nn.Modules that we had also seen in Assignment 2. They might additionally have extra methods such as `VisualizeFilter` in `model.py`.\n","\n","The Trainer glues everything together. It creates the model, sets up optimizer, lr_schduler etc. and has the option to `train()` or `evaluate()` a model over the given dataloaders. It also logs everything in `Logs/YOUR_EXP_NAME.log` and saves the checkpoints under the `Saved/YOUR_EXP_NAME/`. Please familirize yourself with the `__init__` and methods of both `trainers/base_trainer.py` and `trainers/cnn_trainer.py` before continuing with the assignment.\n","\n","Lastly, for tracking different metrics (top(1/5) (train/val) accuracy or losses), we use a MetricTracker object defined in `src/utils/utils.py`. A single tracker keeps track of multiple metric keys and can `update()` their history by adding new values to a list. In the end, it can be used to return an average of a metric.\n","\n","\n","Feel free to ask questions on the forum if part of the codebase is confusing.\n"]},{"cell_type":"markdown","metadata":{"id":"cpyHKxVkc4t-"},"source":["### Question 1: Implement Convolutional Network (10 points)\n","\n","In this question, we will implement a five-layered convolutional neural network architecture as well as the loss function to train it. Refer to the comments in the code to the exact places where you need to fill in the code.\n","\n","![Failed to load the image. Please view it yourself at ./data/exercise-3/fig1_resized.png](./data/exercise-3/fig1_resized.png)"]},{"cell_type":"markdown","metadata":{"id":"AZsjBOeJc4uA"},"source":["Our architecture is shown in Fig 1. It has five convolution blocks. Each block is consist of convolution, max pooling, and ReLU operation in that order. We will use 3×3 kernels in all convolutional layers. Set the padding and stride of the convolutional layers so that they maintain the spatial dimensions. Max pooling operations are done with 2×2 kernels, with a stride of 2, thereby halving the spatial resolution each time. Finally, stacking these five blocks leads to a 512 × 1 × 1 feature map. Classification is achieved by a fully connected layer. We will train convolutional neural networks on the CIFAR-10 dataset. Implement a class ConvNet to define the model described. The ConvNet takes 32 × 32 color images as inputs and has 5 hidden layers with 128, 512, 512, 512, 512 filters, and produces a 10-class classification.\n","\n","a) Please implement the above network (initialization and forward pass) in class `ConvNet` in `models/cnn/model.py`. The code to train the model is already provided in the `trainers/base_trainer.py`'s train() and `trainers/cnn_trainer`'s _train_epoch(). Train the above model and report the training and validation accuracies. (5 points)\n","\n","b) Implement the method `__str__` in `models/base_model.py`, which should give a string representaiton of the model. The string should show the number of `trainable` parameters for each layer. This gives us a measure of model capacity. Also at the end, it should print the total number of trainable parameters for the entire model. (2 points)"]},{"cell_type":"markdown","metadata":{"id":"gyQqsQZVc4uD"},"source":["c) Implement a function `VisualizeFilter` in `models/cnn/model.py`, which visualizes the filters of the first convolution layer implemented in Q1.a. In other words, you need to show 128 filters with size 3x3 as color images (since each filter has three input channels). Stack these into 3x3 color images into one large image. You can use the `imshow` function from the `matplotlib` library to visualize the weights. See an example in Fig. 2\n","\n","![Failed to load the image. Please view it yourself at ./data/exercise-3/fig2_resized.png](./data/exercise-3/fig2_resized.png)"]},{"cell_type":"markdown","metadata":{"id":"p-2AJHblNdBN"},"source":["Compare the filters before and after training. Do you see any patterns? (3 points). Please attach your output images before and after training in a cell with your submission."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4c_YYSTwNdBN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718031289050,"user_tz":-120,"elapsed":13947,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"a20d9ecf-37f4-45a7-c955-fe9b77f7d011"},"outputs":[{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","transforms for preset CIFAR10 for split eval are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Initialization DataLoader for 10000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]}],"source":["from cfgs.exercise_3 import cnn_cifar10\n","q1_config = cnn_cifar10.q1_experiment\n","\n","datamodule_class = q1_config['datamodule']\n","data_args = q1_config['data_args']\n","\n","dm = datamodule_class(**data_args)\n","\n","# Based on the heldout_split in the config file,\n","# the datamodule will break the dataset into two splits\n","train_data_loader = dm.get_loader()\n","valid_data_loader = dm.get_heldout_loader()\n","\n","# Test loader is the same as train loader\n","# except that training=False, shuffle=False, and no splitting is done\n","# So we use the exact config from training and just modify these arguments\n","test_data_args = deepcopy(data_args) # copy the args\n","test_data_args['training'] = False\n","test_data_args['shuffle'] = False\n","test_data_args['heldout_split'] = 0.0\n","\n","# Now we initialize the test module with the modified config\n","test_dm = datamodule_class(**test_data_args)\n","test_loader = test_dm.get_loader() # and get the loader from it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSa6u0o4nJZO"},"outputs":[],"source":["# import shutil\n","# import os\n","\n","# # Function to delete a directory if it exists\n","# def clean_directory(path):\n","#     if os.path.exists(path):\n","#         shutil.rmtree(path)\n","\n","\n","# # Define paths\n","# log_dir = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs'\n","# save_dir = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved'\n","# # Clean up old logs and saved models\n","# clean_directory(log_dir)\n","# clean_directory(save_dir)\n","\n","# # Now re-create the log and save directories\n","# os.makedirs(log_dir, exist_ok=True)\n","# os.makedirs(save_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAKobTq8NdBO","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1718031637865,"user_tz":-120,"elapsed":66819,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"1e75990d-923c-44d1-f4a6-09ca71a9a690"},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 1000x1000 with 128 Axes>"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stderr","text":["  0% 0/45000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Train Epoch: 1 Loss: 0.9749: : 100% 45000/45000 [00:22<00:00, 1971.70it/s]\n","Eval Loss: 1.2292: : 100% 5000/5000 [00:09<00:00, 554.56it/s]\n","Train Epoch: 2 Loss: 1.0071: : 100% 45000/45000 [00:20<00:00, 2150.67it/s]\n","Eval Loss: 1.2431: : 100% 5000/5000 [00:06<00:00, 768.38it/s]\n","Train Epoch: 3 Loss: 0.7527: : 100% 45000/45000 [00:21<00:00, 2136.63it/s]\n","Eval Loss: 0.9801: : 100% 5000/5000 [00:09<00:00, 514.49it/s]\n","Train Epoch: 4 Loss: 0.6392: : 100% 45000/45000 [00:23<00:00, 1946.26it/s]\n","Eval Loss: 1.3756: : 100% 5000/5000 [00:06<00:00, 747.74it/s]\n","Train Epoch: 5 Loss: 0.5672: : 100% 45000/45000 [00:21<00:00, 2049.34it/s]\n","Eval Loss: 0.9195: : 100% 5000/5000 [00:09<00:00, 554.32it/s]\n","Train Epoch: 6 Loss: 0.5695: : 100% 45000/45000 [00:21<00:00, 2124.70it/s]\n","Eval Loss: 0.7417: : 100% 5000/5000 [00:06<00:00, 742.78it/s]\n","Train Epoch: 7 Loss: 0.3380: : 100% 45000/45000 [00:21<00:00, 2065.66it/s]\n","Eval Loss: 0.5736: : 100% 5000/5000 [00:09<00:00, 518.27it/s]\n","Train Epoch: 8 Loss: 0.3983: : 100% 45000/45000 [00:21<00:00, 2089.99it/s]\n","Eval Loss: 0.5800: : 100% 5000/5000 [00:07<00:00, 663.88it/s]\n","Train Epoch: 9 Loss: 0.3325: : 100% 45000/45000 [00:23<00:00, 1883.52it/s]\n","Eval Loss: 0.5816: : 100% 5000/5000 [00:07<00:00, 694.65it/s]\n","Train Epoch: 10 Loss: 0.4113: : 100% 45000/45000 [00:21<00:00, 2053.55it/s]\n","Eval Loss: 0.6402: : 100% 5000/5000 [00:08<00:00, 574.01it/s]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x1000 with 128 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["trainer_class = q1_config['trainer_module']\n","trainer_cnn = trainer_class(\n","    config = q1_config,\n","    log_dir = ospj(PROJECT_ROOT,'Logs'),\n","    train_loader=train_data_loader,\n","    eval_loader=valid_data_loader,\n",")\n","\n","trainer_cnn.model.VisualizeFilter()\n","trainer_cnn.train()\n","trainer_cnn.model.VisualizeFilter()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16647,"status":"ok","timestamp":1718031804761,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"},"user_tz":-120},"id":"KwzBBHvINdBO","outputId":"93542751-fc42-44bc-882c-01f89217b800"},"outputs":[{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.9092: : 100% 10000/10000 [00:16<00:00, 612.42it/s]"]},{"output_type":"stream","name":"stdout","text":["{'loss': 0.7356437945365906, 'top1': 0.7639999999999998, 'top5': 0.9827999999999997}\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Change this to the experiment you want to evaluate\n","path = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_1Q/last_model.pth'\n","trainer_cnn.load_model(path=path)\n","\n","result = trainer_cnn.evaluate(loader=test_loader)\n","\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1718031845004,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"},"user_tz":-120},"id":"bWWWWp-fmP6b","outputId":"a18ade99-5607-429a-de77-dfaba1508989"},"outputs":[{"output_type":"stream","name":"stdout","text":["ConvNet(\n","  (conv_layers): Sequential(\n","    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): ReLU()\n","    (4): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): ReLU()\n","    (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (11): ReLU()\n","    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): ReLU()\n","    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (19): ReLU()\n","  )\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")\n","conv_layers.0.weight: 3456\n","conv_layers.0.bias: 128\n","conv_layers.1.weight: 128\n","conv_layers.1.bias: 128\n","conv_layers.4.weight: 589824\n","conv_layers.4.bias: 512\n","conv_layers.5.weight: 512\n","conv_layers.5.bias: 512\n","conv_layers.8.weight: 2359296\n","conv_layers.8.bias: 512\n","conv_layers.9.weight: 512\n","conv_layers.9.bias: 512\n","conv_layers.12.weight: 2359296\n","conv_layers.12.bias: 512\n","conv_layers.13.weight: 512\n","conv_layers.13.bias: 512\n","conv_layers.16.weight: 2359296\n","conv_layers.16.bias: 512\n","conv_layers.17.weight: 512\n","conv_layers.17.bias: 512\n","fc.weight: 5120\n","fc.bias: 10\n","Total Trainable Parameters: 7682826\n"]}],"source":["# Printing the model summary to see the parameters\n","print(trainer_cnn.model)"]},{"cell_type":"markdown","metadata":{"id":"MKti0Y9ic4uK"},"source":["#Report on Convolutional Filters Before and After Training on CIFAR-10 Dataset\n","\n","##Introduction\n","In this report, we analyze the convolutional filters of the first convolutional layer of a Convolutional Neural Network (CNN) before and after training on the CIFAR-10 dataset. The CIFAR-10 dataset consists of 50000 training images of 32x32 resolution with 10 object classes. The CNN is designed to classify these images into their respective categories.\n","\n","###Filters Before Training\n","Before training, the filters in the first convolutional layer are initialized randomly.These initial filters do not have any specific patterns and look like random noise. Here are some observations of the filters before training:\n","- Random Patterns: The filters appear as random speckles and do not exhibit any coherent structure.\n","\n","- Lack of Features: No discernible edges, textures, or shapes can be identified.\n","\n","###Filters After Training\n","After training, the filters in the first convolutional layer have learned to detect various low-level features from the CIFAR-10 images. These features are crucial for the subsequent layers to build more complex representations of the input data. Here are some key observations:\n","\n","- Edge Detection: Many filters have become edge detectors, highlighting boundaries and edges within the images.\n","- Color Sensitivity: Some filters have specialized in detecting specific colors or color transitions.\n","- Directional Features: Filters may show preference for specific orientations (e.g., horizontal, vertical, or diagonal lines).\n","\n","###Comparative Analysis\n","- Pattern Formation: Before training, filters are random and lack structure. After training, they develop clear patterns that correspond to meaningful features in the input data.\n","- Specialization: Post-training filters show specialization, such as edge detection or color sensitivity, which are crucial for the classification task.\n","- Noise Reduction: The randomness seen in the initial filters is significantly reduced, resulting in more coherent and interpretable filter patterns.\n","\n","### Conclusion\n","The analysis of the first convolutional layer's filters before and after training on the CIFAR-10 dataset reveals a significant transformation. Initially, the filters are random and unstructured. After training, they evolve into specialized feature detectors that are essential for the network's ability to classify images effectively. This change underscores the power of convolutional neural networks to learn and extract meaningful features from raw image data.\n","<br>\n","<br>\n","<br>\n","![Failed to load the image. Please view it yourself at ./data/exercise-3/fig2_resized.png](./data/exercise-3/before_training.png)\n","\n","![Failed to load the image. Please view it yourself at ./data/exercise-3/fig2_resized.png](./data/exercise-3/after_training.png)"]},{"cell_type":"markdown","metadata":{"id":"A5_tJXTmc4uK"},"source":["### Question 2: Improve training of Convolutional Networks (15 points)"]},{"cell_type":"markdown","metadata":{"id":"2n-s9RrvNdBO"},"source":["a) Batch normalization is a widely used operation in neural networks, which will increase the speed of convergence and reach higher performance. You can read the paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” for more theoretical details.\n","In practice, these operations are implemented in most toolboxes, such as PyTorch and TensorFlow. Add batch normalization in the model of Q1.a (You can use PyTorch's implementation). Please keep other hyperparameters the same, but only add batch normalization. The ConvNet with batch normalization still uses the same class with Q1.a but different arguments. Check the code for details. In each block, the computations should be in the order of **[convolution -> batch normalization -> pooling -> ReLU]**. Compare the loss curves and accuracy using batch normalization to its counterpart in Q1.a. (5 points)\n","\n","In order to run this experiment, please create a new config dictionary in `cnn_cifar10.py` under the name `q2a_normalization_experiment` (Hint: most of it should be similar to Q1's config). Don't forget to assign the config a new name, so that it doesn't overwrite previous experiments. Similar to the above cells, import the config and run the experiment.\n","\n","You can also add extra code to `base_trainer.py` or `cnn_trainer.py` so that they return extra information after the training is finished. For example, recall that in assignment 2's `models/twolayernet/model.py` we had a train method that would return the history of loss values, and then in the notebook the history was plotted with matplotlib. Feel free to make adjustments that let you better understand what's happening. This also applies to next questions. Right now the code only uses tensorboard and wandb for plotting (if enabled in config)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYC8zMHtyB5U","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1718032198003,"user_tz":-120,"elapsed":184119,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"10a56d90-3d71-4091-83bf-e755ed128122"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","transforms for preset CIFAR10 for split eval are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 10000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 1000x1000 with 128 Axes>"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stderr","text":["Train Epoch: 1 Loss: 1.0698: : 100% 45000/45000 [00:22<00:00, 2020.86it/s]\n","Eval Loss: 1.2831: : 100% 5000/5000 [00:07<00:00, 691.65it/s]\n","Train Epoch: 2 Loss: 0.8144: : 100% 45000/45000 [00:23<00:00, 1909.86it/s]\n","Eval Loss: 1.3269: : 100% 5000/5000 [00:06<00:00, 716.55it/s]\n","Train Epoch: 3 Loss: 0.8283: : 100% 45000/45000 [00:21<00:00, 2097.56it/s]\n","Eval Loss: 1.2868: : 100% 5000/5000 [00:14<00:00, 350.13it/s]\n","Train Epoch: 4 Loss: 0.6497: : 100% 45000/45000 [00:21<00:00, 2098.66it/s]\n","Eval Loss: 1.1309: : 100% 5000/5000 [00:06<00:00, 759.48it/s]\n","Train Epoch: 5 Loss: 0.5777: : 100% 45000/45000 [00:21<00:00, 2077.09it/s]\n","Eval Loss: 0.6555: : 100% 5000/5000 [00:09<00:00, 519.50it/s]\n","Train Epoch: 6 Loss: 0.5394: : 100% 45000/45000 [00:21<00:00, 2088.49it/s]\n","Eval Loss: 0.6838: : 100% 5000/5000 [00:06<00:00, 762.95it/s]\n","Train Epoch: 7 Loss: 0.4497: : 100% 45000/45000 [00:21<00:00, 2093.42it/s]\n","Eval Loss: 0.6433: : 100% 5000/5000 [00:09<00:00, 536.40it/s]\n","Train Epoch: 8 Loss: 0.5289: : 100% 45000/45000 [00:21<00:00, 2074.47it/s]\n","Eval Loss: 0.6911: : 100% 5000/5000 [00:06<00:00, 715.88it/s]\n","Train Epoch: 9 Loss: 0.3226: : 100% 45000/45000 [00:22<00:00, 1982.98it/s]\n","Eval Loss: 0.6096: : 100% 5000/5000 [00:09<00:00, 502.81it/s]\n","Train Epoch: 10 Loss: 0.4622: : 100% 45000/45000 [00:22<00:00, 1959.56it/s]\n","Eval Loss: 0.6599: : 100% 5000/5000 [00:07<00:00, 676.74it/s]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x1000 with 128 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ConvNet(\n","  (conv_layers): Sequential(\n","    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): ReLU()\n","    (4): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): ReLU()\n","    (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (11): ReLU()\n","    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): ReLU()\n","    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (19): ReLU()\n","  )\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")\n","conv_layers.0.weight: 3456\n","conv_layers.0.bias: 128\n","conv_layers.1.weight: 128\n","conv_layers.1.bias: 128\n","conv_layers.4.weight: 589824\n","conv_layers.4.bias: 512\n","conv_layers.5.weight: 512\n","conv_layers.5.bias: 512\n","conv_layers.8.weight: 2359296\n","conv_layers.8.bias: 512\n","conv_layers.9.weight: 512\n","conv_layers.9.bias: 512\n","conv_layers.12.weight: 2359296\n","conv_layers.12.bias: 512\n","conv_layers.13.weight: 512\n","conv_layers.13.bias: 512\n","conv_layers.16.weight: 2359296\n","conv_layers.16.bias: 512\n","conv_layers.17.weight: 512\n","conv_layers.17.bias: 512\n","fc.weight: 5120\n","fc.bias: 10\n","Total Trainable Parameters: 7682826\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.6965: : 100% 10000/10000 [00:16<00:00, 619.25it/s]"]},{"output_type":"stream","name":"stdout","text":["{'loss': 0.6830441838502884, 'top1': 0.7798000000000002, 'top5': 0.9838}\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from cfgs.exercise_3 import cnn_cifar10\n","q2a_config = cnn_cifar10.q2a_normalization_experiment\n","\n","datamodule_class = q2a_config['datamodule']\n","data_args = q2a_config['data_args']\n","\n","dm = datamodule_class(**data_args)\n","\n","train_data_loader = dm.get_loader()\n","valid_data_loader = dm.get_heldout_loader()\n","\n","\n","test_data_args = deepcopy(data_args) # copy the args\n","test_data_args['training'] = False\n","test_data_args['shuffle'] = False\n","test_data_args['heldout_split'] = 0.0\n","\n","test_dm = datamodule_class(**test_data_args)\n","test_loader = test_dm.get_loader()\n","trainer_class = q2a_config['trainer_module']\n","trainer_cnn = trainer_class(\n","    config = q2a_config,\n","    log_dir = ospj(PROJECT_ROOT,'Logs'),\n","    train_loader=train_data_loader,\n","    eval_loader=valid_data_loader,\n",")\n","\n","trainer_cnn.model.VisualizeFilter()\n","trainer_cnn.train()\n","trainer_cnn.model.VisualizeFilter()\n","\n","path = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_BN2Qa/last_model.pth'\n","\n","trainer_cnn.load_model(path=path)\n","\n","print(trainer_cnn.model)\n","result = trainer_cnn.evaluate(loader=test_loader)\n","\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"QNwU0kBiNdBP"},"source":["#Report on the Effect of Batch Normalization in CNN Training on CIFAR-10 Dataset\n","\n","<br>\n","Both models Q1a and 2a were trained on the CIFAR-10 dataset with the same hyperparameters except for the inclusion of batch normalization in the second model(2a). The loss and accuracy metrics were recorded and compared on test set.<br>\n","\n","Performance Metrics Without Batch Normalization:\n","\n","*   Loss: 0.7356\n","*   Top-1 Accuracy: 76.40%\n","*   Top-5 Accuracy: 98.28%<br>\n","\n","Performance Metrics With Batch Normalization:\n","\n","*   Loss: 0.6830\n","*   Top-1 Accuracy: 77.98%\n","*   Top-5 Accuracy: 98.38%\n","\n","The addition of batch normalization significantly improved the performance of the CNN on the test set. The test loss decreased from 0.7356 to 0.6830, indicating that the model with batch normalization generalized better to unseen data. Furthermore, the top-1 accuracy improved from 76.40% to 77.98%, and the top-5 accuracy saw a slight increase from 98.28% to 98.38%.\n","\n","The batch normalization layers helped in accelerating the convergence of the network. This was evident from the loss logs, where the model with batch normalization showed faster and smoother reduction in training loss compared to the model without batch normalization. The inclusion of batch normalization mitigates the problem of internal covariate shift, allowing the network to learn more stable and meaningful features.\n","\n","**Conclusion:**\n","Adding batch normalization to the CNN significantly improved its performance on the CIFAR-10 set. The model with batch normalization not only achieved a lower test loss but also demonstrated higher accuracy. These results show the effectiveness of batch normalization in accelerating the training process and enhancing the model's generalization capabilities."]},{"cell_type":"markdown","metadata":{"id":"6GjLXD-HNdBP"},"source":["b) Throughout training, we optimize our parameters on the training set. This does not guarantee that with every step we also improve on validation and test set as well! Hence, there is no reason for our latest training checkpoint (the last checkpoint after the last epoch) to be the best to keep. One simple idea is to save a checkpoint of the best model for the validation set throughout the training. Meanining that as the training proceeds, we keep checking our **validation** accuracy after each epoch (or every N epochs) and save the best model. This can mitigate overfitting, as if the model overfits to training data (and accuracy on validation set drops), we would still have access to the best model checkpoint! Note that you **should not** do this on the test set, as we are not alowed to optimize **anything** (including the checkpoint selection) on the test set.\n","\n","For this task, you need add the logic for saving the `best model` during the training. In the `src/trainers/base_trainer`, in method `train()` we already have the call to `self.evaluate()`. All you need to add is to process the returned result (a dictionary of metric_key -> metric_value) and see if you should save a checkpoint of the model. If yes, then you can save a checkpoint at `self.checkpoint_dir` under `best_val_model.pth` or a similar name, using the `save_model()` method. Feel free to define additional class attributes or methods if needed.\n","\n","We also recommend adding a few prints, such as the epochs that you save the best model at. You can also use the `self.logger` object.\n","\n","Please also implement the `should_evaluate()` in the `trainers/base_tariner.py`, which allows for doing the cross-validation evaluation in intervals, based on the config.\n","\n","\n","Increase the training epochs to 50 in Q1.a and Q2.a (simply edit their config dictionaries), and compare the **best model** and **latest model** on the **training set** and **validation set**. Due to the randomness, you can train multiple times to verify and observe overfitting and early stopping. (5 points)\n","\n","\n","Feel free to add any needed train/evaluation code below for this task."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOMzwB8tuGLo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718035357482,"user_tz":-120,"elapsed":1054407,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"6d9c9c0b-830c-4c88-ec88-d6a3fd9fef56"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: 1 Loss: 1.0352: : 100% 45000/45000 [00:23<00:00, 1917.28it/s]\n","Eval Loss: 1.3269: : 100% 5000/5000 [00:12<00:00, 399.14it/s]\n","Train Epoch: 2 Loss: 0.8405: : 100% 45000/45000 [00:22<00:00, 2038.44it/s]\n","Eval Loss: 1.0717: : 100% 5000/5000 [00:07<00:00, 700.40it/s]\n","Train Epoch: 3 Loss: 0.6568: : 100% 45000/45000 [00:23<00:00, 1875.58it/s]\n","Eval Loss: 1.0925: : 100% 5000/5000 [00:08<00:00, 576.35it/s]\n","Train Epoch: 4 Loss: 0.6539: : 100% 45000/45000 [00:22<00:00, 1969.77it/s]\n","Eval Loss: 0.7867: : 100% 5000/5000 [00:08<00:00, 569.07it/s]\n","Train Epoch: 5 Loss: 0.5643: : 100% 45000/45000 [00:23<00:00, 1892.55it/s]\n","Eval Loss: 0.6038: : 100% 5000/5000 [00:07<00:00, 698.09it/s]\n","Train Epoch: 6 Loss: 0.4603: : 100% 45000/45000 [00:24<00:00, 1834.04it/s]\n","Eval Loss: 0.5647: : 100% 5000/5000 [00:10<00:00, 454.76it/s]\n","Train Epoch: 7 Loss: 0.5465: : 100% 45000/45000 [00:23<00:00, 1939.82it/s]\n","Eval Loss: 0.6318: : 100% 5000/5000 [00:07<00:00, 682.66it/s]\n","Train Epoch: 8 Loss: 0.5400: : 100% 45000/45000 [00:23<00:00, 1926.15it/s]\n","Eval Loss: 0.6464: : 100% 5000/5000 [00:07<00:00, 690.45it/s]\n","Train Epoch: 9 Loss: 0.4604: : 100% 45000/45000 [00:22<00:00, 1998.49it/s]\n","Eval Loss: 0.5680: : 100% 5000/5000 [00:09<00:00, 542.76it/s]\n","Train Epoch: 10 Loss: 0.4300: : 100% 45000/45000 [00:22<00:00, 2026.59it/s]\n","Eval Loss: 0.5959: : 100% 5000/5000 [00:06<00:00, 718.94it/s]\n","Train Epoch: 11 Loss: 0.3383: : 100% 45000/45000 [00:22<00:00, 2005.80it/s]\n","Eval Loss: 0.4700: : 100% 5000/5000 [00:09<00:00, 537.01it/s]\n","Train Epoch: 12 Loss: 0.1743: : 100% 45000/45000 [00:22<00:00, 1981.46it/s]\n","Eval Loss: 0.7001: : 100% 5000/5000 [00:07<00:00, 709.81it/s]\n","Train Epoch: 13 Loss: 0.1775: : 100% 45000/45000 [00:22<00:00, 1990.10it/s]\n","Eval Loss: 0.7501: : 100% 5000/5000 [00:09<00:00, 503.27it/s]\n","Train Epoch: 14 Loss: 0.3138: : 100% 45000/45000 [00:22<00:00, 1982.42it/s]\n","Eval Loss: 0.6201: : 100% 5000/5000 [00:07<00:00, 677.36it/s]\n","Train Epoch: 15 Loss: 0.2982: : 100% 45000/45000 [00:24<00:00, 1849.75it/s]\n","Eval Loss: 0.6537: : 100% 5000/5000 [00:10<00:00, 486.36it/s]\n","Train Epoch: 16 Loss: 0.1129: : 100% 45000/45000 [00:23<00:00, 1878.49it/s]\n","Eval Loss: 0.6136: : 100% 5000/5000 [00:07<00:00, 672.97it/s]\n","Train Epoch: 17 Loss: 0.1053: : 100% 45000/45000 [00:23<00:00, 1901.61it/s]\n","Eval Loss: 0.5476: : 100% 5000/5000 [00:09<00:00, 523.09it/s]\n","Train Epoch: 18 Loss: 0.1217: : 100% 45000/45000 [00:22<00:00, 1973.57it/s]\n","Eval Loss: 0.6386: : 100% 5000/5000 [00:07<00:00, 654.43it/s]\n","Train Epoch: 19 Loss: 0.1155: : 100% 45000/45000 [00:22<00:00, 1961.60it/s]\n","Eval Loss: 0.8390: : 100% 5000/5000 [00:07<00:00, 684.57it/s]\n","Train Epoch: 20 Loss: 0.1526: : 100% 45000/45000 [00:22<00:00, 2042.92it/s]\n","Eval Loss: 0.7504: : 100% 5000/5000 [00:09<00:00, 503.41it/s]\n","Train Epoch: 21 Loss: 0.0461: : 100% 45000/45000 [00:22<00:00, 1965.60it/s]\n","Eval Loss: 0.6649: : 100% 5000/5000 [00:07<00:00, 701.93it/s]\n","Train Epoch: 22 Loss: 0.0295: : 100% 45000/45000 [00:22<00:00, 1966.39it/s]\n","Eval Loss: 0.7431: : 100% 5000/5000 [00:10<00:00, 489.29it/s]\n","Train Epoch: 23 Loss: 0.0162: : 100% 45000/45000 [00:22<00:00, 2017.29it/s]\n","Eval Loss: 0.5750: : 100% 5000/5000 [00:07<00:00, 713.79it/s]\n","Train Epoch: 24 Loss: 0.0080: : 100% 45000/45000 [00:22<00:00, 1987.32it/s]\n","Eval Loss: 0.6805: : 100% 5000/5000 [00:09<00:00, 518.96it/s]\n","Train Epoch: 25 Loss: 0.0109: : 100% 45000/45000 [00:22<00:00, 2012.10it/s]\n","Eval Loss: 0.7375: : 100% 5000/5000 [00:07<00:00, 708.52it/s]\n","Train Epoch: 26 Loss: 0.0107: : 100% 45000/45000 [00:22<00:00, 2012.85it/s]\n","Eval Loss: 0.6776: : 100% 5000/5000 [00:09<00:00, 529.42it/s]\n","Train Epoch: 27 Loss: 0.0032: : 100% 45000/45000 [00:21<00:00, 2111.97it/s]\n","Eval Loss: 0.6569: : 100% 5000/5000 [00:06<00:00, 744.22it/s]\n","Train Epoch: 28 Loss: 0.0048: : 100% 45000/45000 [00:21<00:00, 2048.81it/s]\n","Eval Loss: 0.6598: : 100% 5000/5000 [00:09<00:00, 523.96it/s]\n","Train Epoch: 29 Loss: 0.0028: : 100% 45000/45000 [00:21<00:00, 2108.28it/s]\n","Eval Loss: 0.6413: : 100% 5000/5000 [00:06<00:00, 748.57it/s]\n","Train Epoch: 30 Loss: 0.0039: : 100% 45000/45000 [00:21<00:00, 2141.24it/s]\n","Eval Loss: 0.6171: : 100% 5000/5000 [00:08<00:00, 557.86it/s]\n","Train Epoch: 31 Loss: 0.0026: : 100% 45000/45000 [00:20<00:00, 2181.12it/s]\n","Eval Loss: 0.6114: : 100% 5000/5000 [00:06<00:00, 773.35it/s]\n","Train Epoch: 32 Loss: 0.0032: : 100% 45000/45000 [00:21<00:00, 2118.77it/s]\n","Eval Loss: 0.6183: : 100% 5000/5000 [00:09<00:00, 555.53it/s]\n","Train Epoch: 33 Loss: 0.0024: : 100% 45000/45000 [00:20<00:00, 2143.65it/s]\n","Eval Loss: 0.6170: : 100% 5000/5000 [00:06<00:00, 734.34it/s]\n","Train Epoch: 34 Loss: 0.0028: : 100% 45000/45000 [00:20<00:00, 2168.27it/s]\n","Eval Loss: 0.6198: : 100% 5000/5000 [00:09<00:00, 547.98it/s]\n","Train Epoch: 35 Loss: 0.0032: : 100% 45000/45000 [00:21<00:00, 2064.76it/s]\n","Eval Loss: 0.6432: : 100% 5000/5000 [00:06<00:00, 749.65it/s]\n","Train Epoch: 36 Loss: 0.0028: : 100% 45000/45000 [00:21<00:00, 2066.59it/s]\n","Eval Loss: 0.6167: : 100% 5000/5000 [00:09<00:00, 512.01it/s]\n","Train Epoch: 37 Loss: 0.0025: : 100% 45000/45000 [00:22<00:00, 2020.82it/s]\n","Eval Loss: 0.6231: : 100% 5000/5000 [00:06<00:00, 722.03it/s]\n","Train Epoch: 38 Loss: 0.0024: : 100% 45000/45000 [00:21<00:00, 2122.66it/s]\n","Eval Loss: 0.6019: : 100% 5000/5000 [00:09<00:00, 541.04it/s]\n","Train Epoch: 39 Loss: 0.0022: : 100% 45000/45000 [00:21<00:00, 2047.38it/s]\n","Eval Loss: 0.6206: : 100% 5000/5000 [00:07<00:00, 707.41it/s]\n","Train Epoch: 40 Loss: 0.0032: : 100% 45000/45000 [00:21<00:00, 2061.04it/s]\n","Eval Loss: 0.6042: : 100% 5000/5000 [00:09<00:00, 519.62it/s]\n","Train Epoch: 41 Loss: 0.0027: : 100% 45000/45000 [00:21<00:00, 2073.12it/s]\n","Eval Loss: 0.6324: : 100% 5000/5000 [00:06<00:00, 719.86it/s]\n","Train Epoch: 42 Loss: 0.0026: : 100% 45000/45000 [00:21<00:00, 2073.35it/s]\n","Eval Loss: 0.5917: : 100% 5000/5000 [00:08<00:00, 556.09it/s]\n","Train Epoch: 43 Loss: 0.0028: : 100% 45000/45000 [00:21<00:00, 2074.56it/s]\n","Eval Loss: 0.6058: : 100% 5000/5000 [00:06<00:00, 737.64it/s]\n","Train Epoch: 44 Loss: 0.0029: : 100% 45000/45000 [00:21<00:00, 2085.02it/s]\n","Eval Loss: 0.6280: : 100% 5000/5000 [00:09<00:00, 547.37it/s]\n","Train Epoch: 45 Loss: 0.0028: : 100% 45000/45000 [00:21<00:00, 2083.81it/s]\n","Eval Loss: 0.6165: : 100% 5000/5000 [00:06<00:00, 750.88it/s]\n","Train Epoch: 46 Loss: 0.0022: : 100% 45000/45000 [00:21<00:00, 2058.97it/s]\n","Eval Loss: 0.6355: : 100% 5000/5000 [00:08<00:00, 561.89it/s]\n","Train Epoch: 47 Loss: 0.0024: : 100% 45000/45000 [00:22<00:00, 2044.17it/s]\n","Eval Loss: 0.6255: : 100% 5000/5000 [00:06<00:00, 742.31it/s]\n","Train Epoch: 48 Loss: 0.0027: : 100% 45000/45000 [00:21<00:00, 2063.38it/s]\n","Eval Loss: 0.6250: : 100% 5000/5000 [00:08<00:00, 559.16it/s]\n","Train Epoch: 49 Loss: 0.0023: : 100% 45000/45000 [00:21<00:00, 2084.32it/s]\n","Eval Loss: 0.6045: : 100% 5000/5000 [00:06<00:00, 734.68it/s]\n","Train Epoch: 50 Loss: 0.0023: : 100% 45000/45000 [00:21<00:00, 2062.05it/s]\n","Eval Loss: 0.6258: : 100% 5000/5000 [00:08<00:00, 584.49it/s]\n","Train Epoch: 1 Loss: 0.9106: : 100% 45000/45000 [00:21<00:00, 2067.00it/s]\n","Eval Loss: 1.1340: : 100% 5000/5000 [00:06<00:00, 732.55it/s]\n","Train Epoch: 2 Loss: 0.8223: : 100% 45000/45000 [00:23<00:00, 1910.25it/s]\n","Eval Loss: 1.2164: : 100% 5000/5000 [00:09<00:00, 521.35it/s]\n","Train Epoch: 3 Loss: 0.6680: : 100% 45000/45000 [00:21<00:00, 2121.42it/s]\n","Eval Loss: 1.0271: : 100% 5000/5000 [00:06<00:00, 751.81it/s]\n","Train Epoch: 4 Loss: 0.6532: : 100% 45000/45000 [00:21<00:00, 2067.63it/s]\n","Eval Loss: 1.0550: : 100% 5000/5000 [00:09<00:00, 545.65it/s]\n","Train Epoch: 5 Loss: 0.5710: : 100% 45000/45000 [00:21<00:00, 2117.66it/s]\n","Eval Loss: 0.9406: : 100% 5000/5000 [00:06<00:00, 766.76it/s]\n","Train Epoch: 6 Loss: 0.6014: : 100% 45000/45000 [00:21<00:00, 2080.31it/s]\n","Eval Loss: 0.6940: : 100% 5000/5000 [00:09<00:00, 534.14it/s]\n","Train Epoch: 7 Loss: 0.3929: : 100% 45000/45000 [00:21<00:00, 2091.95it/s]\n","Eval Loss: 0.6802: : 100% 5000/5000 [00:06<00:00, 754.50it/s]\n","Train Epoch: 8 Loss: 0.4056: : 100% 45000/45000 [00:21<00:00, 2092.01it/s]\n","Eval Loss: 0.7502: : 100% 5000/5000 [00:09<00:00, 554.14it/s]\n","Train Epoch: 9 Loss: 0.4015: : 100% 45000/45000 [00:21<00:00, 2093.26it/s]\n","Eval Loss: 0.9280: : 100% 5000/5000 [00:06<00:00, 759.97it/s]\n","Train Epoch: 10 Loss: 0.3567: : 100% 45000/45000 [00:20<00:00, 2148.36it/s]\n","Eval Loss: 0.5793: : 100% 5000/5000 [00:09<00:00, 546.44it/s]\n","Train Epoch: 11 Loss: 0.2835: : 100% 45000/45000 [00:21<00:00, 2123.62it/s]\n","Eval Loss: 0.5141: : 100% 5000/5000 [00:06<00:00, 767.28it/s]\n","Train Epoch: 12 Loss: 0.2784: : 100% 45000/45000 [00:21<00:00, 2133.45it/s]\n","Eval Loss: 0.5582: : 100% 5000/5000 [00:09<00:00, 535.88it/s]\n","Train Epoch: 13 Loss: 0.2580: : 100% 45000/45000 [00:21<00:00, 2123.38it/s]\n","Eval Loss: 0.7137: : 100% 5000/5000 [00:06<00:00, 762.43it/s]\n","Train Epoch: 14 Loss: 0.3103: : 100% 45000/45000 [00:20<00:00, 2170.35it/s]\n","Eval Loss: 0.5775: : 100% 5000/5000 [00:09<00:00, 530.45it/s]\n","Train Epoch: 15 Loss: 0.2157: : 100% 45000/45000 [00:21<00:00, 2127.01it/s]\n","Eval Loss: 0.6577: : 100% 5000/5000 [00:06<00:00, 744.27it/s]\n","Train Epoch: 16 Loss: 0.1134: : 100% 45000/45000 [00:21<00:00, 2115.07it/s]\n","Eval Loss: 0.6282: : 100% 5000/5000 [00:08<00:00, 590.30it/s]\n","Train Epoch: 17 Loss: 0.1282: : 100% 45000/45000 [00:21<00:00, 2118.27it/s]\n","Eval Loss: 0.5920: : 100% 5000/5000 [00:06<00:00, 757.32it/s]\n","Train Epoch: 18 Loss: 0.1751: : 100% 45000/45000 [00:21<00:00, 2084.43it/s]\n","Eval Loss: 0.6443: : 100% 5000/5000 [00:09<00:00, 553.45it/s]\n","Train Epoch: 19 Loss: 0.0994: : 100% 45000/45000 [00:21<00:00, 2094.56it/s]\n","Eval Loss: 0.7715: : 100% 5000/5000 [00:06<00:00, 748.63it/s]\n","Train Epoch: 20 Loss: 0.1245: : 100% 45000/45000 [00:21<00:00, 2131.96it/s]\n","Eval Loss: 0.5986: : 100% 5000/5000 [00:09<00:00, 521.50it/s]\n","Train Epoch: 21 Loss: 0.0654: : 100% 45000/45000 [00:21<00:00, 2142.67it/s]\n","Eval Loss: 0.5469: : 100% 5000/5000 [00:06<00:00, 767.55it/s]\n","Train Epoch: 22 Loss: 0.0221: : 100% 45000/45000 [00:21<00:00, 2095.03it/s]\n","Eval Loss: 0.6074: : 100% 5000/5000 [00:09<00:00, 550.82it/s]\n","Train Epoch: 23 Loss: 0.0124: : 100% 45000/45000 [00:21<00:00, 2109.33it/s]\n","Eval Loss: 0.6554: : 100% 5000/5000 [00:06<00:00, 768.45it/s]\n","Train Epoch: 24 Loss: 0.0192: : 100% 45000/45000 [00:21<00:00, 2095.31it/s]\n","Eval Loss: 0.6139: : 100% 5000/5000 [00:09<00:00, 537.74it/s]\n","Train Epoch: 25 Loss: 0.0248: : 100% 45000/45000 [00:21<00:00, 2141.01it/s]\n","Eval Loss: 0.6208: : 100% 5000/5000 [00:06<00:00, 763.34it/s]\n","Train Epoch: 26 Loss: 0.0469: : 100% 45000/45000 [00:21<00:00, 2096.74it/s]\n","Eval Loss: 0.5806: : 100% 5000/5000 [00:09<00:00, 554.50it/s]\n","Train Epoch: 27 Loss: 0.0593: : 100% 45000/45000 [00:21<00:00, 2112.66it/s]\n","Eval Loss: 0.6697: : 100% 5000/5000 [00:06<00:00, 772.55it/s]\n","Train Epoch: 28 Loss: 0.0942: : 100% 45000/45000 [00:21<00:00, 2117.62it/s]\n","Eval Loss: 0.7036: : 100% 5000/5000 [00:08<00:00, 560.46it/s]\n","Train Epoch: 29 Loss: 0.1425: : 100% 45000/45000 [00:21<00:00, 2137.86it/s]\n","Eval Loss: 0.5864: : 100% 5000/5000 [00:06<00:00, 763.04it/s]\n","Train Epoch: 30 Loss: 0.0634: : 100% 45000/45000 [00:21<00:00, 2136.92it/s]\n","Eval Loss: 0.5946: : 100% 5000/5000 [00:09<00:00, 542.89it/s]\n","Train Epoch: 31 Loss: 0.0249: : 100% 45000/45000 [00:21<00:00, 2129.12it/s]\n","Eval Loss: 0.4846: : 100% 5000/5000 [00:06<00:00, 770.90it/s]\n","Train Epoch: 32 Loss: 0.0049: : 100% 45000/45000 [00:21<00:00, 2132.54it/s]\n","Eval Loss: 0.5049: : 100% 5000/5000 [00:09<00:00, 520.83it/s]\n","Train Epoch: 33 Loss: 0.0052: : 100% 45000/45000 [00:22<00:00, 2015.56it/s]\n","Eval Loss: 0.4654: : 100% 5000/5000 [00:06<00:00, 724.60it/s]\n","Train Epoch: 34 Loss: 0.0031: : 100% 45000/45000 [00:22<00:00, 2040.76it/s]\n","Eval Loss: 0.5070: : 100% 5000/5000 [00:09<00:00, 501.02it/s]\n","Train Epoch: 35 Loss: 0.0025: : 100% 45000/45000 [00:21<00:00, 2091.24it/s]\n","Eval Loss: 0.4986: : 100% 5000/5000 [00:06<00:00, 720.50it/s]\n","Train Epoch: 36 Loss: 0.0037: : 100% 45000/45000 [00:22<00:00, 1961.63it/s]\n","Eval Loss: 0.5151: : 100% 5000/5000 [00:09<00:00, 526.87it/s]\n","Train Epoch: 37 Loss: 0.0020: : 100% 45000/45000 [00:22<00:00, 2008.25it/s]\n","Eval Loss: 0.5044: : 100% 5000/5000 [00:07<00:00, 694.71it/s]\n","Train Epoch: 38 Loss: 0.0025: : 100% 45000/45000 [00:23<00:00, 1906.85it/s]\n","Eval Loss: 0.5069: : 100% 5000/5000 [00:09<00:00, 530.05it/s]\n","Train Epoch: 39 Loss: 0.0031: : 100% 45000/45000 [00:22<00:00, 1962.59it/s]\n","Eval Loss: 0.5093: : 100% 5000/5000 [00:07<00:00, 689.52it/s]\n","Train Epoch: 40 Loss: 0.0030: : 100% 45000/45000 [00:23<00:00, 1937.59it/s]\n","Eval Loss: 0.5073: : 100% 5000/5000 [00:06<00:00, 719.11it/s]\n","Train Epoch: 41 Loss: 0.0020: : 100% 45000/45000 [00:22<00:00, 1992.88it/s]\n","Eval Loss: 0.5042: : 100% 5000/5000 [00:09<00:00, 538.94it/s]\n","Train Epoch: 42 Loss: 0.0020: : 100% 45000/45000 [00:22<00:00, 2010.23it/s]\n","Eval Loss: 0.5149: : 100% 5000/5000 [00:06<00:00, 726.81it/s]\n","Train Epoch: 43 Loss: 0.0028: : 100% 45000/45000 [00:22<00:00, 2009.31it/s]\n","Eval Loss: 0.5072: : 100% 5000/5000 [00:09<00:00, 518.55it/s]\n","Train Epoch: 44 Loss: 0.0023: : 100% 45000/45000 [00:22<00:00, 2029.75it/s]\n","Eval Loss: 0.5185: : 100% 5000/5000 [00:06<00:00, 724.90it/s]\n","Train Epoch: 45 Loss: 0.0025: : 100% 45000/45000 [00:22<00:00, 1957.93it/s]\n","Eval Loss: 0.5145: : 100% 5000/5000 [00:09<00:00, 538.75it/s]\n","Train Epoch: 46 Loss: 0.0027: : 100% 45000/45000 [00:22<00:00, 2013.49it/s]\n","Eval Loss: 0.5166: : 100% 5000/5000 [00:07<00:00, 663.27it/s]\n","Train Epoch: 47 Loss: 0.0041: : 100% 45000/45000 [00:22<00:00, 1969.89it/s]\n","Eval Loss: 0.5147: : 100% 5000/5000 [00:07<00:00, 682.78it/s]\n","Train Epoch: 48 Loss: 0.0021: : 100% 45000/45000 [00:22<00:00, 2007.67it/s]\n","Eval Loss: 0.5244: : 100% 5000/5000 [00:09<00:00, 550.17it/s]\n","Train Epoch: 49 Loss: 0.0023: : 100% 45000/45000 [00:22<00:00, 1996.15it/s]\n","Eval Loss: 0.5121: : 100% 5000/5000 [00:06<00:00, 723.29it/s]\n","Train Epoch: 50 Loss: 0.0019: : 100% 45000/45000 [00:22<00:00, 1993.88it/s]\n","Eval Loss: 0.5134: : 100% 5000/5000 [00:10<00:00, 497.62it/s]\n"]}],"source":["from cfgs.exercise_3 import cnn_cifar10\n","q1_config = cnn_cifar10.q2b_experiment1\n","q2_config = cnn_cifar10.q2b_experiment2\n","\n","datamodule_class = q1_config['datamodule']\n","data_args = q1_config['data_args']\n","\n","dm = datamodule_class(**data_args)\n","\n","train_data_loader = dm.get_loader()\n","valid_data_loader = dm.get_heldout_loader()\n","\n","\n","# Train Q1 model\n","trainer_class = q1_config['trainer_module']\n","trainer_cnn = trainer_class(\n","    config=q1_config,\n","    log_dir = ospj(PROJECT_ROOT,'Logs'),\n","    train_loader=train_data_loader,\n","    eval_loader=valid_data_loader,\n",")\n","trainer_cnn.train()\n","\n","# Train Q2 model with batch normalization\n","trainer_class = q2_config['trainer_module']\n","trainer_cnn_bn = trainer_class(\n","    config=q2_config,\n","    log_dir = ospj(PROJECT_ROOT,'Logs'),\n","    train_loader=train_data_loader,\n","    eval_loader=valid_data_loader,\n",")\n","trainer_cnn_bn.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqLmUMrrxJSg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718035683199,"user_tz":-120,"elapsed":324255,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"eb884a0a-84d4-4383-a7e2-eda7ffa973e4"},"outputs":[{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.6413: : 100% 5000/5000 [00:12<00:00, 412.89it/s]\n","Eval Loss: 0.0014: : 100% 45000/45000 [01:16<00:00, 589.29it/s]\n","Eval Loss: 0.6258: : 100% 5000/5000 [00:07<00:00, 694.12it/s]\n","Eval Loss: 0.0017: : 100% 45000/45000 [01:11<00:00, 627.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Q1 Best Model Results - Validation: {'loss': 0.6517983615398407, 'top1': 0.8386, 'top5': 0.9882}\n","Q1 Best Model Results - train: {'loss': 0.001502085639577773, 'top1': 1.0, 'top5': 1.0}\n","Q1 Latest Model Results - Validation: {'loss': 0.6357011902332306, 'top1': 0.8251999999999998, 'top5': 0.985}\n","Q1 Latest Model Results - Train: {'loss': 0.0018215539963502023, 'top1': 1.0, 'top5': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.5073: : 100% 5000/5000 [00:08<00:00, 604.86it/s]\n","Eval Loss: 0.0015: : 100% 45000/45000 [01:08<00:00, 657.41it/s]\n","Eval Loss: 0.5134: : 100% 5000/5000 [00:09<00:00, 550.33it/s]\n","Eval Loss: 0.0019: : 100% 45000/45000 [01:10<00:00, 639.99it/s]"]},{"output_type":"stream","name":"stdout","text":["Q2 Best Model Results - Validation: {'loss': 0.6311970639228821, 'top1': 0.8365999999999998, 'top5': 0.9865999999999998}\n","Q2 Best Model Results - train: {'loss': 0.0016546897476332055, 'top1': 1.0, 'top5': 1.0}\n","Q2 latest Model Results - Validation: {'loss': 0.6190809416770935, 'top1': 0.8344, 'top5': 0.9849999999999999}\n","Q2 Latest Model Results - train: {'loss': 0.001749516809359193, 'top1': 1.0, 'top5': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Load best model\n","best_model_path_q1 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_best_model2b/best_val_model.pth'\n","trainer_cnn.load_model(path=best_model_path_q1)\n","best_model_results_q1_val = trainer_cnn.evaluate(loader=valid_data_loader)\n","best_model_results_q1_train = trainer_cnn.evaluate(loader=train_data_loader)\n","\n","# Load latest model\n","latest_model_path_q1 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_best_model2b/last_model.pth'\n","trainer_cnn.load_model(path=latest_model_path_q1)\n","latest_model_results_q1_val = trainer_cnn.evaluate(loader=valid_data_loader)\n","latest_model_results_q1_train = trainer_cnn.evaluate(loader=train_data_loader)\n","\n","# Print results\n","print(\"Q1 Best Model Results - Validation:\", best_model_results_q1_val)\n","print(\"Q1 Best Model Results - train:\", best_model_results_q1_train)\n","print(\"Q1 Latest Model Results - Validation:\", latest_model_results_q1_val)\n","print(\"Q1 Latest Model Results - Train:\", latest_model_results_q1_train)\n","\n","# Load best model\n","best_model_path_q2 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_best_modelBN2b/best_val_model.pth'\n","trainer_cnn_bn.load_model(path=best_model_path_q2)\n","best_model_results_q2_val = trainer_cnn_bn.evaluate(loader=valid_data_loader)\n","best_model_results_q2_train = trainer_cnn_bn.evaluate(loader=train_data_loader)\n","\n","# Load latest model\n","latest_model_path_q2 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_best_modelBN2b/last_model.pth'\n","trainer_cnn_bn.load_model(path=latest_model_path_q2)\n","latest_model_results_q2_val = trainer_cnn_bn.evaluate(loader=valid_data_loader)\n","latest_model_results_q2_train = trainer_cnn_bn.evaluate(loader=train_data_loader)\n","# Print results\n","print(\"Q2 Best Model Results - Validation:\", best_model_results_q2_val)\n","print(\"Q2 Best Model Results - train:\", best_model_results_q2_train)\n","print(\"Q2 latest Model Results - Validation:\", latest_model_results_q2_val)\n","print(\"Q2 Latest Model Results - train:\", latest_model_results_q2_train)\n"]},{"cell_type":"markdown","metadata":{"id":"wWeRAObUc4uL"},"source":["\n","#Report on the Effect of Saving Best Validation Model Checkpoints in CNN Training on CIFAR-10 Dataset\n","\n","**Introduction**\n","Throughout training, we optimize our model parameters on the training set. However, improvements on the training set do not always translate to better performance on the validation or test sets. To mitigate this, we can save a checkpoint of the best model based on validation accuracy throughout the training process. This approach ensures that we retain the best-performing model on the validation set, which can help mitigate overfitting. This report compares the best model and the latest model based on their performance on the training and validation sets for two CNN architectures (Q1.a and Q2.a).<br>\n","\n","**Results** <br>\n","Q1.a (Without Batch Normalization)\n","Best Model at Epoch 29:<br>\n","Training Set:\n","* Loss: 0.00150\n","* Top-1 Accuracy: 100.00%\n","* Top-5 Accuracy: 100.00%\n","\n","Validation Set:\n","* Loss: 0.6518\n","* Top-1 Accuracy: 83.86%\n","* Top-5 Accuracy: 98.82%\n","\n","Latest Model:(Epoch 50)<br>\n","Training Set:\n","* Loss: 0.00182\n","* Top-1 Accuracy: 100.00%\n","* Top-5 Accuracy: 100.00%\n","\n","Validation Set:\n","* Loss: 0.6357\n","* Top-1 Accuracy: 82.52%\n","* Top-5 Accuracy: 98.50%\n","\n","Q2.a (With Batch Normalization)\n","Best Model at Epoch 40:<br>\n","Training Set:\n","* Loss: 0.00165\n","* Top-1 Accuracy: 100.00%\n","* Top-5 Accuracy: 100.00%\n","\n","Validation Set:\n","* Loss: 0.6312\n","* Top-1 Accuracy: 83.66%\n","* Top-5 Accuracy: 98.66%\n","\n","Latest Model(Epoch 50):\n","\n","Training Set:\n","* Loss: 0.00175\n","* Top-1 Accuracy: 100.00%\n","* Top-5 Accuracy: 100.00%\n","\n","Validation Set:\n","* Loss: 0.6191\n","* Top-1 Accuracy: 83.44%\n","* Top-5 Accuracy: 98.50%\n","<br>\n","\n","For both models (Q1.a without batch normalization and Q2.a with batch normalization), the best model based on validation accuracy showed slightly better or comparable performance on the validation set compared to the latest model.<br>\n","\n","Q1.a\n","* The best model at epoch 29 had a validation loss of 0.6518 and a top-1 accuracy of 83.86%, compared to the latest model's validation loss of 0.6357 and top-1 accuracy of 82.52%.\n","This indicates that the best model achieved better top-1 accuracy on the validation set, despite having a slightly higher loss than the latest model.\n","\n","Q2.a\n","* The best model at epoch 40 had a validation loss of 0.6312 and a top-1 accuracy of 83.66%, compared to the latest model's validation loss of 0.6191 and top-1 accuracy of 83.44%.\n","The best model showed marginally better top-1 accuracy, although the latest model had a slightly lower validation loss.\n","\n","**Conclusion**:\n","Saving the best model checkpoint based on validation accuracy throughout the training process is a useful strategy to ensure that the model generalizes well to unseen data. Both Q1.a and Q2.a demonstrated that the best models, as identified during training, had better or comparable validation performance than the latest models. This approach helps mitigate overfitting and ensures that the model retained for testing is the most performant on the validation set.\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"fL2OQp9hNdBP"},"source":["c) While in part `b` we save the best model, we still do as many epochs as indicated in the config file. This is not convenient as the overfitting steps are wasting time and compute and also wouldn't affect the best model. Hence, Early Stopping can be helpful, where we **stop** the training after a few non-improving steps! Early stopping logic should be considered after every training epoch is finished, to see if we should do more epochs or not. Therefore, the logic should should be implemented ath the end of the loop over epochs in the `train()` method of `base_trainer.py` (which takes care of running multiple epochs).\n","\n","Once implemented, you need a new config dictionary to enable early stopping. Simply create a new one at the bottom of `cfgs/exercise-3/cnn_cifar10.py`. It should be mostly similar to previous config, with the following modification:\n","```Python\n","q2c_earlystop_experiment = dict(\n","    name = 'Some New Name' # Otherwise it will overwrite previous experiment!\n","    ...\n","    trainer = dict(\n","        ...\n","        monitor = \"off\", # -> chante to \"max eval_top1\"\n","        early_stop = 0, #  -> change to 4\n","    ),\n",")\n","```\n","This will enable the early stopping to be considered for `eval_top1` metric and the maximum number of non-improving steps will be set to 4.\n","\n","Use the cells below to re-run one of the experiments from part `b` that the best epoch was way lower than the total number of epochs, and see if early stopping can prevent unnecessary training epochs in that case."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dTnHvtxUESsf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718037064751,"user_tz":-120,"elapsed":1176122,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"b06e2ebc-c2e8-4e8a-a40a-f778025ca9b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","transforms for preset CIFAR10 for split eval are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 10000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: 1 Loss: 1.1028: : 100% 45000/45000 [00:21<00:00, 2093.02it/s]\n","Eval Loss: 1.0884: : 100% 5000/5000 [00:06<00:00, 763.25it/s]\n","Train Epoch: 2 Loss: 0.8378: : 100% 45000/45000 [00:25<00:00, 1764.32it/s]\n","Eval Loss: 1.1843: : 100% 5000/5000 [00:08<00:00, 584.04it/s]\n","Train Epoch: 3 Loss: 0.8109: : 100% 45000/45000 [00:21<00:00, 2061.30it/s]\n","Eval Loss: 0.9746: : 100% 5000/5000 [00:07<00:00, 645.58it/s]\n","Train Epoch: 4 Loss: 0.6612: : 100% 45000/45000 [00:22<00:00, 2009.51it/s]\n","Eval Loss: 0.8561: : 100% 5000/5000 [00:06<00:00, 726.13it/s]\n","Train Epoch: 5 Loss: 0.7506: : 100% 45000/45000 [00:22<00:00, 2027.01it/s]\n","Eval Loss: 0.7284: : 100% 5000/5000 [00:10<00:00, 494.36it/s]\n","Train Epoch: 6 Loss: 0.5036: : 100% 45000/45000 [00:22<00:00, 2041.69it/s]\n","Eval Loss: 0.7294: : 100% 5000/5000 [00:06<00:00, 715.08it/s]\n","Train Epoch: 7 Loss: 0.4359: : 100% 45000/45000 [00:21<00:00, 2047.99it/s]\n","Eval Loss: 0.5515: : 100% 5000/5000 [00:09<00:00, 525.84it/s]\n","Train Epoch: 8 Loss: 0.4445: : 100% 45000/45000 [00:21<00:00, 2102.29it/s]\n","Eval Loss: 0.6244: : 100% 5000/5000 [00:06<00:00, 730.22it/s]\n","Train Epoch: 9 Loss: 0.5212: : 100% 45000/45000 [00:21<00:00, 2088.19it/s]\n","Eval Loss: 0.6988: : 100% 5000/5000 [00:09<00:00, 533.83it/s]\n","Train Epoch: 10 Loss: 0.3673: : 100% 45000/45000 [00:21<00:00, 2127.78it/s]\n","Eval Loss: 0.5975: : 100% 5000/5000 [00:06<00:00, 756.04it/s]\n","Train Epoch: 11 Loss: 0.2445: : 100% 45000/45000 [00:22<00:00, 2034.84it/s]\n","Eval Loss: 0.6286: : 100% 5000/5000 [00:09<00:00, 547.05it/s]\n","Train Epoch: 12 Loss: 0.2886: : 100% 45000/45000 [00:21<00:00, 2127.66it/s]\n","Eval Loss: 0.6677: : 100% 5000/5000 [00:06<00:00, 739.24it/s]\n","Train Epoch: 13 Loss: 0.2625: : 100% 45000/45000 [00:22<00:00, 2030.84it/s]\n","Eval Loss: 0.5182: : 100% 5000/5000 [00:08<00:00, 555.61it/s]\n","Train Epoch: 14 Loss: 0.2593: : 100% 45000/45000 [00:21<00:00, 2118.57it/s]\n","Eval Loss: 0.6319: : 100% 5000/5000 [00:08<00:00, 568.57it/s]\n","Train Epoch: 15 Loss: 0.2552: : 100% 45000/45000 [00:21<00:00, 2081.93it/s]\n","Eval Loss: 0.6576: : 100% 5000/5000 [00:06<00:00, 768.00it/s]\n","Train Epoch: 16 Loss: 0.1401: : 100% 45000/45000 [00:21<00:00, 2134.22it/s]\n","Eval Loss: 0.6097: : 100% 5000/5000 [00:07<00:00, 642.30it/s]\n","Train Epoch: 17 Loss: 0.1471: : 100% 45000/45000 [00:21<00:00, 2141.82it/s]\n","Eval Loss: 0.7491: : 100% 5000/5000 [00:06<00:00, 773.32it/s]\n","Train Epoch: 18 Loss: 0.2062: : 100% 45000/45000 [00:20<00:00, 2165.03it/s]\n","Eval Loss: 0.7708: : 100% 5000/5000 [00:08<00:00, 569.41it/s]\n","Train Epoch: 19 Loss: 0.1758: : 100% 45000/45000 [00:20<00:00, 2174.45it/s]\n","Eval Loss: 0.7883: : 100% 5000/5000 [00:06<00:00, 768.17it/s]\n","Train Epoch: 20 Loss: 0.1275: : 100% 45000/45000 [00:20<00:00, 2159.16it/s]\n","Eval Loss: 0.6130: : 100% 5000/5000 [00:08<00:00, 564.82it/s]\n","Train Epoch: 1 Loss: 0.8899: : 100% 45000/45000 [00:20<00:00, 2148.58it/s]\n","Eval Loss: 1.1243: : 100% 5000/5000 [00:06<00:00, 781.05it/s]\n","Train Epoch: 2 Loss: 0.9908: : 100% 45000/45000 [00:21<00:00, 2057.71it/s]\n","Eval Loss: 1.0421: : 100% 5000/5000 [00:09<00:00, 551.80it/s]\n","Train Epoch: 3 Loss: 0.7496: : 100% 45000/45000 [00:21<00:00, 2137.78it/s]\n","Eval Loss: 1.1753: : 100% 5000/5000 [00:06<00:00, 777.98it/s]\n","Train Epoch: 4 Loss: 0.6793: : 100% 45000/45000 [00:21<00:00, 2127.61it/s]\n","Eval Loss: 0.9452: : 100% 5000/5000 [00:08<00:00, 584.30it/s]\n","Train Epoch: 5 Loss: 0.5730: : 100% 45000/45000 [00:21<00:00, 2127.33it/s]\n","Eval Loss: 0.9952: : 100% 5000/5000 [00:06<00:00, 721.66it/s]\n","Train Epoch: 6 Loss: 0.7092: : 100% 45000/45000 [00:21<00:00, 2079.31it/s]\n","Eval Loss: 0.8502: : 100% 5000/5000 [00:06<00:00, 770.01it/s]\n","Train Epoch: 7 Loss: 0.4928: : 100% 45000/45000 [00:20<00:00, 2164.31it/s]\n","Eval Loss: 0.6561: : 100% 5000/5000 [00:08<00:00, 624.30it/s]\n","Train Epoch: 8 Loss: 0.6030: : 100% 45000/45000 [00:20<00:00, 2182.07it/s]\n","Eval Loss: 0.7263: : 100% 5000/5000 [00:06<00:00, 773.13it/s]\n","Train Epoch: 9 Loss: 0.4216: : 100% 45000/45000 [00:21<00:00, 2072.47it/s]\n","Eval Loss: 0.6314: : 100% 5000/5000 [00:08<00:00, 599.92it/s]\n","Train Epoch: 10 Loss: 0.3723: : 100% 45000/45000 [00:21<00:00, 2088.89it/s]\n","Eval Loss: 0.7137: : 100% 5000/5000 [00:06<00:00, 766.89it/s]\n","Train Epoch: 11 Loss: 0.2417: : 100% 45000/45000 [00:21<00:00, 2127.54it/s]\n","Eval Loss: 0.5825: : 100% 5000/5000 [00:08<00:00, 578.38it/s]\n","Train Epoch: 12 Loss: 0.2529: : 100% 45000/45000 [00:21<00:00, 2095.66it/s]\n","Eval Loss: 0.8631: : 100% 5000/5000 [00:06<00:00, 736.41it/s]\n","Train Epoch: 13 Loss: 0.3101: : 100% 45000/45000 [00:21<00:00, 2112.71it/s]\n","Eval Loss: 0.6386: : 100% 5000/5000 [00:08<00:00, 563.63it/s]\n","Train Epoch: 14 Loss: 0.1995: : 100% 45000/45000 [00:21<00:00, 2089.26it/s]\n","Eval Loss: 0.6566: : 100% 5000/5000 [00:06<00:00, 744.98it/s]\n","Train Epoch: 15 Loss: 0.2047: : 100% 45000/45000 [00:20<00:00, 2189.87it/s]\n","Eval Loss: 0.8154: : 100% 5000/5000 [00:09<00:00, 547.20it/s]\n","Train Epoch: 16 Loss: 0.1225: : 100% 45000/45000 [00:21<00:00, 2138.06it/s]\n","Eval Loss: 0.6519: : 100% 5000/5000 [00:06<00:00, 761.24it/s]\n","Train Epoch: 17 Loss: 0.0840: : 100% 45000/45000 [00:21<00:00, 2096.08it/s]\n","Eval Loss: 0.6913: : 100% 5000/5000 [00:08<00:00, 592.32it/s]\n","Train Epoch: 18 Loss: 0.1475: : 100% 45000/45000 [00:21<00:00, 2133.61it/s]\n","Eval Loss: 0.8265: : 100% 5000/5000 [00:06<00:00, 768.56it/s]\n","Train Epoch: 19 Loss: 0.1811: : 100% 45000/45000 [00:21<00:00, 2105.26it/s]\n","Eval Loss: 0.7081: : 100% 5000/5000 [00:08<00:00, 560.86it/s]\n","Train Epoch: 20 Loss: 0.1988: : 100% 45000/45000 [00:21<00:00, 2130.20it/s]\n","Eval Loss: 0.6607: : 100% 5000/5000 [00:06<00:00, 754.15it/s]\n"]}],"source":["from cfgs.exercise_3 import cnn_cifar10\n","q1_config = cnn_cifar10.q2c_earlystop_experiment1\n","q2_config = cnn_cifar10.q2c_earlystop_experiment2\n","\n","datamodule_class = q1_config['datamodule']\n","data_args = q1_config['data_args']\n","\n","dm = datamodule_class(**data_args)\n","\n","train_data_loader = dm.get_loader()\n","valid_data_loader = dm.get_heldout_loader()\n","\n","test_data_args = deepcopy(data_args) # copy the args\n","test_data_args['training'] = False\n","test_data_args['shuffle'] = False\n","test_data_args['heldout_split'] = 0.0\n","\n","# Now we initialize the test module with the modified config\n","test_dm = datamodule_class(**test_data_args)\n","test_loader = test_dm.get_loader()\n","\n","\n","# Train Q1 model\n","trainer_class = q1_config['trainer_module']\n","trainer_cnn = trainer_class(\n","    config=q1_config,\n","    log_dir = ospj(PROJECT_ROOT,'Logs'),\n","    train_loader=train_data_loader,\n","    eval_loader=valid_data_loader,\n",")\n","trainer_cnn.train()\n","\n","# Train Q2 model with batch normalization\n","trainer_class = q2_config['trainer_module']\n","trainer_cnn_bn = trainer_class(\n","    config=q2_config,\n","    log_dir = ospj(PROJECT_ROOT,'Logs'),\n","    train_loader=train_data_loader,\n","    eval_loader=valid_data_loader,\n",")\n","trainer_cnn_bn.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjZHIWOU1_5h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718037245907,"user_tz":-120,"elapsed":44087,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"572a8b52-be77-44ee-9d47-91d68df63d36"},"outputs":[{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.7420: : 100% 10000/10000 [00:13<00:00, 757.23it/s]\n","Eval Loss: 0.6267: : 100% 10000/10000 [00:14<00:00, 687.52it/s]\n","Eval Loss: 0.6097: : 100% 5000/5000 [00:09<00:00, 549.93it/s]\n","Eval Loss: 0.6519: : 100% 5000/5000 [00:06<00:00, 770.67it/s]"]},{"output_type":"stream","name":"stdout","text":["Result of early stop without batch norm on Validation set: {'loss': 0.6888891744613648, 'top1': 0.8146, 'top5': 0.988}\n","Result of early stop with batch norm on Validation set: {'loss': 0.6768863952159881, 'top1': 0.8168000000000001, 'top5': 0.9887999999999999}\n","Result of early stop without batch norm on test set: {'loss': 0.7111110937595367, 'top1': 0.8086999999999998, 'top5': 0.9878999999999993}\n","Result of early stop with batch norm on test set: {'loss': 0.7132325208187104, 'top1': 0.8079999999999999, 'top5': 0.9882999999999998}\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["\n","path1 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_EarlyStopQ1_2c/best_val_model.pth'\n","path2=   '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Q2_C_Q2BN_2c/best_val_model.pth'\n","trainer_cnn.load_model(path=path1)\n","trainer_cnn_bn.load_model(path=path2)\n","# Evaluate the model\n","result1 = trainer_cnn.evaluate(loader=test_loader)\n","result2=trainer_cnn_bn.evaluate(loader=test_loader)\n","result1Val = trainer_cnn.evaluate(loader=valid_data_loader)\n","result2Val=trainer_cnn_bn.evaluate(loader=valid_data_loader)\n","\n","print(\"Result of early stop without batch norm on Validation set:\",result1Val)\n","print(\"Result of early stop with batch norm on Validation set:\",result2Val)\n","print(\"Result of early stop without batch norm on test set:\",result1)\n","print(\"Result of early stop with batch norm on test set:\",result2)"]},{"cell_type":"markdown","metadata":{"id":"DmGENcKYNdBP"},"source":["Write your report for Q2.c in this cell.Feel free to add extra code cells\n","\n","#Report on Implementing Early Stopping in CNN Training on CIFAR-10 Dataset\n","<br>\n","\n","**Introduction**:\n","* While saving the best model during training is beneficial, continuing the training for the full number of epochs specified in the configuration file can be inefficient, especially if the model starts to overfit. Early stopping is a technique that can help mitigate this by halting training once performance on the validation set stops improving. This report evaluates the performance of CNN models trained with and without batch normalization(Q1a and Q2a), using early stopping to terminate training after non-improving epochs.\n","\n","\n","**Method**:\n","* The early stopping mechanism was implemented to monitor the validation accuracy at the end of each epoch. If the validation accuracy did not improve for 4 consecutive epochs, training was halted. This approach was applied to both CNN models (with and without batch normalization). Training was stopped at 20 epochs due to early stopping, rather than continuing for the full training cycle of 50 epochs.\n","\n","**Results**:\n","* Early Stopping Without Batch Normalization[**Best Model Found at Epoch 16**]:\n"," * Validation Set:\n","   * Loss: 0.6889\n","   * Top-1 Accuracy: 81.46%\n","   * Top-5 Accuracy: 98.80%\n","  * Test Set:\n","   * Loss: 0.7111\n","   * Top-1 Accuracy: 80.87%\n","   * Top-5 Accuracy: 98.79%\n","\n","* Early Stopping With Batch Normalization[**Best Model found at Epoch 16**]\n"," * Validation Set:\n","   * Loss: 0.6769\n","   * Top-1 Accuracy: 81.68%\n","   * Top-5 Accuracy: 98.88%\n"," * Test Set:\n","   * Loss: 0.7132\n","   * Top-1 Accuracy: 80.80%\n","   * Top-5 Accuracy: 98.83%\n","\n","**Analysis:**\n","* The early stopping mechanism was effective in both models, halting training at 20 epochs instead of continuing for the full training cycle. This not only saved computational resources but also helped in avoiding overfitting.\n","\n","**Performance Without Batch Normalization**:\n","* The model achieved a validation loss of 0.6889 with a top-1 accuracy of 81.46% and a top-5 accuracy of 98.80%.\n","* On the test set, the model recorded a loss of 0.7111, a top-1 accuracy of 80.87%, and a top-5 accuracy of 98.79%.\n","\n","**Performance With Batch Normalization**:\n","* The model achieved a validation loss of 0.6769 with a top-1 accuracy of 81.68% and a top-5 accuracy of 98.88%.\n","* On the test set, the model recorded a loss of 0.7132, a top-1 accuracy of 80.80%, and a top-5 accuracy of 98.83%.\n","\n","**Conclusion**:\n","* Early stopping proved to be an effective strategy for optimizing the training process by terminating epochs when no improvement in validation accuracy was observed. This method not only enhanced computational efficiency but also maintained or slightly improved model performance. Both CNN models, with and without batch normalization, demonstrated strong performance on the CIFAR-10 dataset when early stopping was applied. Notably, the model with batch normalization showed marginally better results in terms of validation loss and top-1 accuracy compared to the model without batch normalization. These findings support the use of early stopping as a practical technique to prevent overfitting and to enhance the efficiency of the training process.\n","\n","<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"qc1RwzMnc4uL"},"source":["### Question 3: Improve generalization of Convolutional Networks (10 points)\n","\n","We saw in Q2 that the model can start over-fitting to the training set if we continue training for long. To prevent over-fitting, there are two main paradigms we can focus on.\n","\n","The first is to get more training data. This might be a difficult and expensive process. However, it is generally the most effective way to learn more general models. A cheaper alternative is to perform data augmentation. The second approach is to regularize the model to prevent overfitting.\n","\n","In the following sub-questions, we will experiment with each of these paradigms and measure the effect on the model generalization. We recommend disabling Early Stopping from previous question (simply removing it from config file) so that it does not interrupt your experiments with data augmentations and you maintain full control over number of epochs.\n","\n","\n","\n","a) Data augmentation is the process of creating more training data by applying certain transformations to the training set images. Usually, the underlying assumption is that the label of the image does not change under the applied transformations. This includes geometric transformations like translation, rotation, scaling, flipping, random cropping, and color transformations like greyscale, colorjitter. For every image in the training batch, a random transformation is sampled from the possible ones (e.g., a random number of pixels to translate the image by) and is applied to the image. While designing the data input pipeline, we must choose the hyper-parameters for these transformations (e.g., limits of translation or rotation) based on things we expect to see in the test-set/real world. Your task in this question is to implement the data augmentation for the CIFAR-10 classification task. Many of these transformations are implemented in the `torchvision.transforms` package. Familiarize yourself with the APIs of these transforms, and functions to compose multiple transforms or randomly sample them. Next, implement geometric and color space data augmentations for the CIFAR-10 dataset, by choosing the right functions and order of application. Tune the hyper-parameters of these data augmentations to improve the validation performance. You will need to train the model a bit longer (20-30 epochs) with data augmentation, as the training data is effectively larger now. Discuss which augmentations work well for you in the report. (6 points)\n","\n","Create as many config dictionaries as you need in `cnn_cifar10.py`. For every augmentation, simply create a new preset under `src/utils/transform_presets.py` and reference its name in your experiment's config dict.\n","\n","\n","\n","b) Dropout is a popular scheme to regularize the model to improve generalization. The dropout layer works by setting the input activations randomly to zero at the output. You can implement Dropout by adding the `torch.nn.Dropout` layer between the conv blocks in your model. The layer has a single hyper-parameter $p$, which is the probability of dropping the input activations. High values of $p$ regularize the model heavily and decrease model capacity, but with low values, the model might overfit. Find the right hyper-parameter for $p$ by training the model for different values of $p$ and comparing training validation and validation accuracies. You can use the same parameter $p$ for all layers. You can also disable the data augmentation from the previous step while running this experiment, to clearly see the benefit of dropout. Show the plot of training and validation accuracies for different values of dropout (0.1 - 0.9) in the report. Create as many config dictionaries as you need in `cnn_cifar10.py`. (4 points)"]},{"cell_type":"code","source":["# a\n","from cfgs.exercise_3 import cnn_cifar10\n","q3a_aug1_config = cnn_cifar10.q3a_aug1_experiment\n","q3a_aug2_config = cnn_cifar10.q3a_aug2_experiment\n","q3a_aug3_config = cnn_cifar10.q3a_aug3_experiment\n","q3a_aug4_config = cnn_cifar10.q3a_aug4_experiment\n","\n","def create_trainer(config):\n","    datamodule_class = config['datamodule']\n","    data_args = config['data_args']\n","\n","    dm = datamodule_class(**data_args)\n","\n","    train_data_loader = dm.get_loader()\n","    valid_data_loader = dm.get_heldout_loader()\n","\n","    trainer_class = config['trainer_module']\n","    trainer = trainer_class(\n","        config=config,\n","        log_dir='/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs',\n","        train_loader=train_data_loader,\n","        eval_loader=valid_data_loader,\n","    )\n","    return trainer\n","\n","trainer_aug1 = create_trainer(q3a_aug1_config)\n","trainer_aug2 = create_trainer(q3a_aug2_config)\n","trainer_aug3 = create_trainer(q3a_aug3_config)\n","trainer_aug4 = create_trainer(q3a_aug4_config)"],"metadata":{"id":"p_s2SDpBBjWN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718037468175,"user_tz":-120,"elapsed":7986,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"d26d6c62-aa9d-4835-8013-227a78753f93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10_WithFlip for split train are Compose(\n","    RandomHorizontalFlip(p=0.5)\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","transforms for preset CIFAR10_WithRotation for split train are Compose(\n","    RandomHorizontalFlip(p=0.5)\n","    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","transforms for preset CIFAR10_WithColorJitter for split train are Compose(\n","    RandomHorizontalFlip(p=0.5)\n","    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.2, 0.2))\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","transforms for preset CIFAR10_WithAllAug for split train are Compose(\n","    RandomHorizontalFlip(p=0.5)\n","    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n","    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.2, 0.2))\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]}]},{"cell_type":"code","source":["trainer_aug1.train()\n","trainer_aug2.train()\n","trainer_aug3.train()\n","trainer_aug4.train()"],"metadata":{"id":"LtwV1cCrJNa0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718043208026,"user_tz":-120,"elapsed":4856754,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"cd768e86-09ba-46a9-b538-bff8c9ecae27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Train Epoch: 1 Loss: 0.9321: : 100% 45000/45000 [00:22<00:00, 2008.40it/s]\n","Eval Loss: 1.4931: : 100% 5000/5000 [00:06<00:00, 721.04it/s]\n","Train Epoch: 2 Loss: 0.9157: : 100% 45000/45000 [00:25<00:00, 1753.44it/s]\n","Eval Loss: 1.5628: : 100% 5000/5000 [00:06<00:00, 749.26it/s]\n","Train Epoch: 3 Loss: 0.8837: : 100% 45000/45000 [00:22<00:00, 2041.25it/s]\n","Eval Loss: 1.3023: : 100% 5000/5000 [00:08<00:00, 563.40it/s]\n","Train Epoch: 4 Loss: 0.7452: : 100% 45000/45000 [00:22<00:00, 2032.03it/s]\n","Eval Loss: 0.7020: : 100% 5000/5000 [00:08<00:00, 582.06it/s]\n","Train Epoch: 5 Loss: 0.9500: : 100% 45000/45000 [00:23<00:00, 1931.47it/s]\n","Eval Loss: 0.7447: : 100% 5000/5000 [00:06<00:00, 746.57it/s]\n","Train Epoch: 6 Loss: 0.4874: : 100% 45000/45000 [00:22<00:00, 2036.68it/s]\n","Eval Loss: 0.6285: : 100% 5000/5000 [00:09<00:00, 554.63it/s]\n","Train Epoch: 7 Loss: 0.6622: : 100% 45000/45000 [00:21<00:00, 2083.57it/s]\n","Eval Loss: 0.6943: : 100% 5000/5000 [00:06<00:00, 755.59it/s]\n","Train Epoch: 8 Loss: 0.5746: : 100% 45000/45000 [00:22<00:00, 1963.19it/s]\n","Eval Loss: 0.6336: : 100% 5000/5000 [00:07<00:00, 632.37it/s]\n","Train Epoch: 9 Loss: 0.4231: : 100% 45000/45000 [00:21<00:00, 2053.08it/s]\n","Eval Loss: 0.5348: : 100% 5000/5000 [00:07<00:00, 642.40it/s]\n","Train Epoch: 10 Loss: 0.4973: : 100% 45000/45000 [00:21<00:00, 2048.76it/s]\n","Eval Loss: 0.5091: : 100% 5000/5000 [00:06<00:00, 754.09it/s]\n","Train Epoch: 11 Loss: 0.3647: : 100% 45000/45000 [00:22<00:00, 2028.85it/s]\n","Eval Loss: 0.5260: : 100% 5000/5000 [00:08<00:00, 572.36it/s]\n","Train Epoch: 12 Loss: 0.3763: : 100% 45000/45000 [00:21<00:00, 2072.77it/s]\n","Eval Loss: 0.6408: : 100% 5000/5000 [00:07<00:00, 684.72it/s]\n","Train Epoch: 13 Loss: 0.3051: : 100% 45000/45000 [00:22<00:00, 1961.95it/s]\n","Eval Loss: 0.5602: : 100% 5000/5000 [00:06<00:00, 747.63it/s]\n","Train Epoch: 14 Loss: 0.2788: : 100% 45000/45000 [00:24<00:00, 1813.05it/s]\n","Eval Loss: 0.6033: : 100% 5000/5000 [00:07<00:00, 652.25it/s]\n","Train Epoch: 15 Loss: 0.3557: : 100% 45000/45000 [00:22<00:00, 2025.00it/s]\n","Eval Loss: 0.6458: : 100% 5000/5000 [00:06<00:00, 754.49it/s]\n","Train Epoch: 16 Loss: 0.2115: : 100% 45000/45000 [00:22<00:00, 2040.48it/s]\n","Eval Loss: 0.5085: : 100% 5000/5000 [00:09<00:00, 531.75it/s]\n","Train Epoch: 17 Loss: 0.2923: : 100% 45000/45000 [00:22<00:00, 2039.07it/s]\n","Eval Loss: 0.5424: : 100% 5000/5000 [00:07<00:00, 713.32it/s]\n","Train Epoch: 18 Loss: 0.1667: : 100% 45000/45000 [00:22<00:00, 1956.57it/s]\n","Eval Loss: 0.5175: : 100% 5000/5000 [00:06<00:00, 746.69it/s]\n","Train Epoch: 19 Loss: 0.3114: : 100% 45000/45000 [00:22<00:00, 2045.28it/s]\n","Eval Loss: 0.5014: : 100% 5000/5000 [00:09<00:00, 529.68it/s]\n","Train Epoch: 20 Loss: 0.2048: : 100% 45000/45000 [00:21<00:00, 2076.30it/s]\n","Eval Loss: 0.5557: : 100% 5000/5000 [00:06<00:00, 761.03it/s]\n","Train Epoch: 21 Loss: 0.1890: : 100% 45000/45000 [00:22<00:00, 2039.11it/s]\n","Eval Loss: 0.5145: : 100% 5000/5000 [00:09<00:00, 528.89it/s]\n","Train Epoch: 22 Loss: 0.2141: : 100% 45000/45000 [00:21<00:00, 2075.16it/s]\n","Eval Loss: 0.4724: : 100% 5000/5000 [00:06<00:00, 720.50it/s]\n","Train Epoch: 23 Loss: 0.2515: : 100% 45000/45000 [00:24<00:00, 1803.39it/s]\n","Eval Loss: 0.4605: : 100% 5000/5000 [00:08<00:00, 586.84it/s]\n","Train Epoch: 24 Loss: 0.1425: : 100% 45000/45000 [00:21<00:00, 2052.33it/s]\n","Eval Loss: 0.4992: : 100% 5000/5000 [00:07<00:00, 663.04it/s]\n","Train Epoch: 25 Loss: 0.1801: : 100% 45000/45000 [00:22<00:00, 2019.09it/s]\n","Eval Loss: 0.4405: : 100% 5000/5000 [00:06<00:00, 753.16it/s]\n","Train Epoch: 26 Loss: 0.0939: : 100% 45000/45000 [00:21<00:00, 2052.98it/s]\n","Eval Loss: 0.4913: : 100% 5000/5000 [00:09<00:00, 536.64it/s]\n","Train Epoch: 27 Loss: 0.1258: : 100% 45000/45000 [00:22<00:00, 2040.06it/s]\n","Eval Loss: 0.5705: : 100% 5000/5000 [00:06<00:00, 722.47it/s]\n","Train Epoch: 28 Loss: 0.0943: : 100% 45000/45000 [00:22<00:00, 1972.81it/s]\n","Eval Loss: 0.3978: : 100% 5000/5000 [00:06<00:00, 746.32it/s]\n","Train Epoch: 29 Loss: 0.1291: : 100% 45000/45000 [00:21<00:00, 2073.56it/s]\n","Eval Loss: 0.4784: : 100% 5000/5000 [00:09<00:00, 541.73it/s]\n","Train Epoch: 30 Loss: 0.0909: : 100% 45000/45000 [00:21<00:00, 2047.48it/s]\n","Eval Loss: 0.5305: : 100% 5000/5000 [00:06<00:00, 741.79it/s]\n","Train Epoch: 1 Loss: 1.0935: : 100% 45000/45000 [00:26<00:00, 1691.44it/s]\n","Eval Loss: 1.3324: : 100% 5000/5000 [00:09<00:00, 503.83it/s]\n","Train Epoch: 2 Loss: 0.9879: : 100% 45000/45000 [00:26<00:00, 1674.07it/s]\n","Eval Loss: 1.3052: : 100% 5000/5000 [00:07<00:00, 690.96it/s]\n","Train Epoch: 3 Loss: 0.9841: : 100% 45000/45000 [00:26<00:00, 1689.10it/s]\n","Eval Loss: 1.1156: : 100% 5000/5000 [00:07<00:00, 697.12it/s]\n","Train Epoch: 4 Loss: 0.8023: : 100% 45000/45000 [00:24<00:00, 1811.93it/s]\n","Eval Loss: 0.8173: : 100% 5000/5000 [00:09<00:00, 510.47it/s]\n","Train Epoch: 5 Loss: 0.7149: : 100% 45000/45000 [00:24<00:00, 1823.08it/s]\n","Eval Loss: 0.7609: : 100% 5000/5000 [00:09<00:00, 538.24it/s]\n","Train Epoch: 6 Loss: 0.6902: : 100% 45000/45000 [00:24<00:00, 1810.98it/s]\n","Eval Loss: 0.6935: : 100% 5000/5000 [00:07<00:00, 666.91it/s]\n","Train Epoch: 7 Loss: 0.5328: : 100% 45000/45000 [00:26<00:00, 1672.51it/s]\n","Eval Loss: 0.7756: : 100% 5000/5000 [00:07<00:00, 685.47it/s]\n","Train Epoch: 8 Loss: 0.6332: : 100% 45000/45000 [00:25<00:00, 1782.28it/s]\n","Eval Loss: 0.7158: : 100% 5000/5000 [00:07<00:00, 678.42it/s]\n","Train Epoch: 9 Loss: 0.5244: : 100% 45000/45000 [00:24<00:00, 1830.10it/s]\n","Eval Loss: 0.5959: : 100% 5000/5000 [00:11<00:00, 431.62it/s]\n","Train Epoch: 10 Loss: 0.5548: : 100% 45000/45000 [00:24<00:00, 1838.78it/s]\n","Eval Loss: 0.5769: : 100% 5000/5000 [00:08<00:00, 565.41it/s]\n","Train Epoch: 11 Loss: 0.5598: : 100% 45000/45000 [00:24<00:00, 1834.84it/s]\n","Eval Loss: 0.5851: : 100% 5000/5000 [00:07<00:00, 650.12it/s]\n","Train Epoch: 12 Loss: 0.3859: : 100% 45000/45000 [00:26<00:00, 1723.81it/s]\n","Eval Loss: 0.6969: : 100% 5000/5000 [00:07<00:00, 712.32it/s]\n","Train Epoch: 13 Loss: 0.4102: : 100% 45000/45000 [00:26<00:00, 1708.99it/s]\n","Eval Loss: 0.5743: : 100% 5000/5000 [00:06<00:00, 715.90it/s]\n","Train Epoch: 14 Loss: 0.5420: : 100% 45000/45000 [00:25<00:00, 1780.42it/s]\n","Eval Loss: 0.5250: : 100% 5000/5000 [00:09<00:00, 500.40it/s]\n","Train Epoch: 15 Loss: 0.5152: : 100% 45000/45000 [00:24<00:00, 1832.57it/s]\n","Eval Loss: 0.4635: : 100% 5000/5000 [00:09<00:00, 545.78it/s]\n","Train Epoch: 16 Loss: 0.4387: : 100% 45000/45000 [00:25<00:00, 1763.53it/s]\n","Eval Loss: 0.4825: : 100% 5000/5000 [00:07<00:00, 645.87it/s]\n","Train Epoch: 17 Loss: 0.3615: : 100% 45000/45000 [00:28<00:00, 1557.64it/s]\n","Eval Loss: 0.5058: : 100% 5000/5000 [00:07<00:00, 660.84it/s]\n","Train Epoch: 18 Loss: 0.4246: : 100% 45000/45000 [00:26<00:00, 1669.99it/s]\n","Eval Loss: 0.5672: : 100% 5000/5000 [00:07<00:00, 682.30it/s]\n","Train Epoch: 19 Loss: 0.3700: : 100% 45000/45000 [00:26<00:00, 1713.61it/s]\n","Eval Loss: 0.5756: : 100% 5000/5000 [00:10<00:00, 483.09it/s]\n","Train Epoch: 20 Loss: 0.4657: : 100% 45000/45000 [00:26<00:00, 1712.09it/s]\n","Eval Loss: 0.5362: : 100% 5000/5000 [00:09<00:00, 520.06it/s]\n","Train Epoch: 21 Loss: 0.3977: : 100% 45000/45000 [00:26<00:00, 1724.20it/s]\n","Eval Loss: 0.4706: : 100% 5000/5000 [00:08<00:00, 614.50it/s]\n","Train Epoch: 22 Loss: 0.3528: : 100% 45000/45000 [00:26<00:00, 1729.53it/s]\n","Eval Loss: 0.5302: : 100% 5000/5000 [00:07<00:00, 633.23it/s]\n","Train Epoch: 23 Loss: 0.3435: : 100% 45000/45000 [00:25<00:00, 1774.56it/s]\n","Eval Loss: 0.4539: : 100% 5000/5000 [00:07<00:00, 653.71it/s]\n","Train Epoch: 24 Loss: 0.3910: : 100% 45000/45000 [00:27<00:00, 1652.45it/s]\n","Eval Loss: 0.4924: : 100% 5000/5000 [00:07<00:00, 696.99it/s]\n","Train Epoch: 25 Loss: 0.3815: : 100% 45000/45000 [00:28<00:00, 1602.91it/s]\n","Eval Loss: 0.5114: : 100% 5000/5000 [00:10<00:00, 497.86it/s]\n","Train Epoch: 26 Loss: 0.2138: : 100% 45000/45000 [00:24<00:00, 1837.94it/s]\n","Eval Loss: 0.5029: : 100% 5000/5000 [00:08<00:00, 623.70it/s]\n","Train Epoch: 27 Loss: 0.2091: : 100% 45000/45000 [00:25<00:00, 1752.62it/s]\n","Eval Loss: 0.4534: : 100% 5000/5000 [00:07<00:00, 665.79it/s]\n","Train Epoch: 28 Loss: 0.3446: : 100% 45000/45000 [00:28<00:00, 1582.55it/s]\n","Eval Loss: 0.4656: : 100% 5000/5000 [00:07<00:00, 637.50it/s]\n","Train Epoch: 29 Loss: 0.2387: : 100% 45000/45000 [00:28<00:00, 1595.73it/s]\n","Eval Loss: 0.5635: : 100% 5000/5000 [00:07<00:00, 632.07it/s]\n","Train Epoch: 30 Loss: 0.2488: : 100% 45000/45000 [00:28<00:00, 1565.35it/s]\n","Eval Loss: 0.5309: : 100% 5000/5000 [00:10<00:00, 456.28it/s]\n","Train Epoch: 1 Loss: 1.0856: : 100% 45000/45000 [00:49<00:00, 900.26it/s] \n","Eval Loss: 1.4389: : 100% 5000/5000 [00:17<00:00, 285.14it/s]\n","Train Epoch: 2 Loss: 0.9567: : 100% 45000/45000 [00:49<00:00, 905.97it/s] \n","Eval Loss: 1.0927: : 100% 5000/5000 [00:10<00:00, 495.34it/s]\n","Train Epoch: 3 Loss: 0.8991: : 100% 45000/45000 [00:53<00:00, 843.89it/s] \n","Eval Loss: 0.9300: : 100% 5000/5000 [00:09<00:00, 501.37it/s]\n","Train Epoch: 4 Loss: 0.9020: : 100% 45000/45000 [00:52<00:00, 854.62it/s] \n","Eval Loss: 0.8306: : 100% 5000/5000 [00:10<00:00, 499.08it/s]\n","Train Epoch: 5 Loss: 0.5964: : 100% 45000/45000 [00:50<00:00, 885.41it/s] \n","Eval Loss: 0.9948: : 100% 5000/5000 [00:10<00:00, 487.85it/s]\n","Train Epoch: 6 Loss: 0.6127: : 100% 45000/45000 [00:51<00:00, 870.16it/s] \n","Eval Loss: 0.6854: : 100% 5000/5000 [00:12<00:00, 388.73it/s]\n","Train Epoch: 7 Loss: 0.5703: : 100% 45000/45000 [00:48<00:00, 936.74it/s] \n","Eval Loss: 0.7137: : 100% 5000/5000 [00:13<00:00, 370.52it/s]\n","Train Epoch: 8 Loss: 0.6360: : 100% 45000/45000 [00:47<00:00, 940.39it/s] \n","Eval Loss: 0.6574: : 100% 5000/5000 [00:13<00:00, 376.68it/s]\n","Train Epoch: 9 Loss: 0.4942: : 100% 45000/45000 [00:47<00:00, 943.94it/s] \n","Eval Loss: 0.5929: : 100% 5000/5000 [00:12<00:00, 397.25it/s]\n","Train Epoch: 10 Loss: 0.5209: : 100% 45000/45000 [00:50<00:00, 889.43it/s] \n","Eval Loss: 0.5017: : 100% 5000/5000 [00:11<00:00, 424.25it/s]\n","Train Epoch: 11 Loss: 0.4912: : 100% 45000/45000 [00:45<00:00, 986.79it/s] \n","Eval Loss: 0.5773: : 100% 5000/5000 [00:12<00:00, 408.00it/s]\n","Train Epoch: 12 Loss: 0.4675: : 100% 45000/45000 [00:45<00:00, 980.50it/s] \n","Eval Loss: 0.5844: : 100% 5000/5000 [00:11<00:00, 417.22it/s]\n","Train Epoch: 13 Loss: 0.4680: : 100% 45000/45000 [00:45<00:00, 984.56it/s] \n","Eval Loss: 0.7157: : 100% 5000/5000 [00:12<00:00, 395.89it/s]\n","Train Epoch: 14 Loss: 0.3954: : 100% 45000/45000 [00:50<00:00, 888.24it/s] \n","Eval Loss: 0.6123: : 100% 5000/5000 [00:09<00:00, 532.29it/s]\n","Train Epoch: 15 Loss: 0.4450: : 100% 45000/45000 [00:46<00:00, 959.51it/s] \n","Eval Loss: 0.6028: : 100% 5000/5000 [00:08<00:00, 574.36it/s]\n","Train Epoch: 16 Loss: 0.3085: : 100% 45000/45000 [00:47<00:00, 941.23it/s] \n","Eval Loss: 0.5471: : 100% 5000/5000 [00:09<00:00, 548.85it/s]\n","Train Epoch: 17 Loss: 0.3391: : 100% 45000/45000 [00:47<00:00, 947.27it/s] \n","Eval Loss: 0.5329: : 100% 5000/5000 [00:09<00:00, 540.18it/s]\n","Train Epoch: 18 Loss: 0.2863: : 100% 45000/45000 [00:50<00:00, 884.18it/s] \n","Eval Loss: 0.6004: : 100% 5000/5000 [00:09<00:00, 548.91it/s]\n","Train Epoch: 19 Loss: 0.3378: : 100% 45000/45000 [00:55<00:00, 815.95it/s] \n","Eval Loss: 0.6897: : 100% 5000/5000 [00:09<00:00, 536.92it/s]\n","Train Epoch: 20 Loss: 0.2452: : 100% 45000/45000 [00:51<00:00, 879.65it/s] \n","Eval Loss: 0.5386: : 100% 5000/5000 [00:09<00:00, 531.01it/s]\n","Train Epoch: 21 Loss: 0.2720: : 100% 45000/45000 [00:50<00:00, 895.48it/s] \n","Eval Loss: 0.5212: : 100% 5000/5000 [00:09<00:00, 519.90it/s]\n","Train Epoch: 22 Loss: 0.1946: : 100% 45000/45000 [00:50<00:00, 891.84it/s] \n","Eval Loss: 0.5527: : 100% 5000/5000 [00:10<00:00, 498.61it/s]\n","Train Epoch: 23 Loss: 0.2565: : 100% 45000/45000 [00:51<00:00, 867.43it/s] \n","Eval Loss: 0.6081: : 100% 5000/5000 [00:11<00:00, 449.92it/s]\n","Train Epoch: 24 Loss: 0.2626: : 100% 45000/45000 [00:47<00:00, 944.18it/s] \n","Eval Loss: 0.4499: : 100% 5000/5000 [00:12<00:00, 412.90it/s]\n","Train Epoch: 25 Loss: 0.2653: : 100% 45000/45000 [00:47<00:00, 948.57it/s] \n","Eval Loss: 0.5872: : 100% 5000/5000 [00:12<00:00, 392.33it/s]\n","Train Epoch: 26 Loss: 0.1030: : 100% 45000/45000 [00:47<00:00, 954.37it/s] \n","Eval Loss: 0.5530: : 100% 5000/5000 [00:13<00:00, 379.76it/s]\n","Train Epoch: 27 Loss: 0.2466: : 100% 45000/45000 [00:51<00:00, 876.65it/s] \n","Eval Loss: 0.5647: : 100% 5000/5000 [00:13<00:00, 384.33it/s]\n","Train Epoch: 28 Loss: 0.1406: : 100% 45000/45000 [00:47<00:00, 947.61it/s] \n","Eval Loss: 0.5004: : 100% 5000/5000 [00:12<00:00, 403.99it/s]\n","Train Epoch: 29 Loss: 0.1890: : 100% 45000/45000 [00:47<00:00, 951.69it/s] \n","Eval Loss: 0.5647: : 100% 5000/5000 [00:12<00:00, 400.40it/s]\n","Train Epoch: 30 Loss: 0.1428: : 100% 45000/45000 [00:47<00:00, 952.37it/s] \n","Eval Loss: 0.6105: : 100% 5000/5000 [00:12<00:00, 409.42it/s]\n","Train Epoch: 1 Loss: 1.2576: : 100% 45000/45000 [00:55<00:00, 810.82it/s] \n","Eval Loss: 1.5229: : 100% 5000/5000 [00:09<00:00, 535.72it/s]\n","Train Epoch: 2 Loss: 1.0038: : 100% 45000/45000 [00:54<00:00, 823.25it/s] \n","Eval Loss: 1.3381: : 100% 5000/5000 [00:10<00:00, 481.17it/s]\n","Train Epoch: 3 Loss: 0.9206: : 100% 45000/45000 [00:49<00:00, 902.12it/s] \n","Eval Loss: 1.2783: : 100% 5000/5000 [00:13<00:00, 371.62it/s]\n","Train Epoch: 4 Loss: 0.8302: : 100% 45000/45000 [00:51<00:00, 874.43it/s] \n","Eval Loss: 1.0873: : 100% 5000/5000 [00:09<00:00, 519.71it/s]\n","Train Epoch: 5 Loss: 0.7787: : 100% 45000/45000 [00:57<00:00, 788.64it/s] \n","Eval Loss: 0.8913: : 100% 5000/5000 [00:10<00:00, 483.36it/s]\n","Train Epoch: 6 Loss: 0.6575: : 100% 45000/45000 [00:49<00:00, 905.61it/s] \n","Eval Loss: 0.7546: : 100% 5000/5000 [00:13<00:00, 369.41it/s]\n","Train Epoch: 7 Loss: 0.6521: : 100% 45000/45000 [00:51<00:00, 882.00it/s] \n","Eval Loss: 0.6941: : 100% 5000/5000 [00:09<00:00, 527.14it/s]\n","Train Epoch: 8 Loss: 0.8107: : 100% 45000/45000 [00:53<00:00, 833.99it/s] \n","Eval Loss: 0.7494: : 100% 5000/5000 [00:10<00:00, 482.28it/s]\n","Train Epoch: 9 Loss: 0.5969: : 100% 45000/45000 [00:52<00:00, 850.86it/s] \n","Eval Loss: 0.7342: : 100% 5000/5000 [00:11<00:00, 420.31it/s]\n","Train Epoch: 10 Loss: 0.5880: : 100% 45000/45000 [00:53<00:00, 847.64it/s] \n","Eval Loss: 0.6250: : 100% 5000/5000 [00:09<00:00, 541.69it/s]\n","Train Epoch: 11 Loss: 0.6957: : 100% 45000/45000 [00:53<00:00, 835.55it/s] \n","Eval Loss: 0.6522: : 100% 5000/5000 [00:10<00:00, 488.30it/s]\n","Train Epoch: 12 Loss: 0.7486: : 100% 45000/45000 [00:51<00:00, 875.10it/s] \n","Eval Loss: 0.5074: : 100% 5000/5000 [00:13<00:00, 373.16it/s]\n","Train Epoch: 13 Loss: 0.6052: : 100% 45000/45000 [01:00<00:00, 741.36it/s] \n","Eval Loss: 0.5194: : 100% 5000/5000 [00:11<00:00, 452.32it/s]\n","Train Epoch: 14 Loss: 0.5071: : 100% 45000/45000 [00:49<00:00, 900.48it/s] \n","Eval Loss: 0.5889: : 100% 5000/5000 [00:13<00:00, 373.08it/s]\n","Train Epoch: 15 Loss: 0.5572: : 100% 45000/45000 [00:52<00:00, 857.55it/s] \n","Eval Loss: 0.6237: : 100% 5000/5000 [00:09<00:00, 534.24it/s]\n","Train Epoch: 16 Loss: 0.5760: : 100% 45000/45000 [00:56<00:00, 795.60it/s] \n","Eval Loss: 0.4812: : 100% 5000/5000 [00:12<00:00, 411.94it/s]\n","Train Epoch: 17 Loss: 0.5442: : 100% 45000/45000 [00:52<00:00, 853.38it/s] \n","Eval Loss: 0.5193: : 100% 5000/5000 [00:12<00:00, 404.48it/s]\n","Train Epoch: 18 Loss: 0.5647: : 100% 45000/45000 [00:56<00:00, 794.73it/s] \n","Eval Loss: 0.4877: : 100% 5000/5000 [00:10<00:00, 486.21it/s]\n","Train Epoch: 19 Loss: 0.5236: : 100% 45000/45000 [00:56<00:00, 799.77it/s] \n","Eval Loss: 0.5392: : 100% 5000/5000 [00:13<00:00, 359.22it/s]\n","Train Epoch: 20 Loss: 0.4724: : 100% 45000/45000 [00:55<00:00, 816.01it/s]\n","Eval Loss: 0.5992: : 100% 5000/5000 [00:09<00:00, 510.20it/s]\n","Train Epoch: 21 Loss: 0.3633: : 100% 45000/45000 [00:55<00:00, 806.14it/s] \n","Eval Loss: 0.5843: : 100% 5000/5000 [00:11<00:00, 421.47it/s]\n","Train Epoch: 22 Loss: 0.3974: : 100% 45000/45000 [00:52<00:00, 865.38it/s] \n","Eval Loss: 0.4446: : 100% 5000/5000 [00:12<00:00, 385.09it/s]\n","Train Epoch: 23 Loss: 0.3482: : 100% 45000/45000 [00:56<00:00, 799.33it/s] \n","Eval Loss: 0.4655: : 100% 5000/5000 [00:10<00:00, 495.09it/s]\n","Train Epoch: 24 Loss: 0.4128: : 100% 45000/45000 [00:55<00:00, 816.72it/s] \n","Eval Loss: 0.4453: : 100% 5000/5000 [00:12<00:00, 385.04it/s]\n","Train Epoch: 25 Loss: 0.3047: : 100% 45000/45000 [00:53<00:00, 835.94it/s]\n","Eval Loss: 0.5158: : 100% 5000/5000 [00:09<00:00, 510.65it/s]\n","Train Epoch: 26 Loss: 0.2814: : 100% 45000/45000 [00:56<00:00, 795.82it/s] \n","Eval Loss: 0.4937: : 100% 5000/5000 [00:12<00:00, 413.57it/s]\n","Train Epoch: 27 Loss: 0.3305: : 100% 45000/45000 [00:52<00:00, 857.82it/s] \n","Eval Loss: 0.4935: : 100% 5000/5000 [00:12<00:00, 388.73it/s]\n","Train Epoch: 28 Loss: 0.3952: : 100% 45000/45000 [00:55<00:00, 812.37it/s] \n","Eval Loss: 0.4284: : 100% 5000/5000 [00:09<00:00, 532.37it/s]\n","Train Epoch: 29 Loss: 0.3377: : 100% 45000/45000 [00:55<00:00, 811.45it/s] \n","Eval Loss: 0.4342: : 100% 5000/5000 [00:12<00:00, 414.10it/s]\n","Train Epoch: 30 Loss: 0.3832: : 100% 45000/45000 [00:51<00:00, 877.16it/s] \n","Eval Loss: 0.5480: : 100% 5000/5000 [00:12<00:00, 391.15it/s]\n"]}]},{"cell_type":"code","source":["test_data_args = deepcopy(data_args) # copy the args\n","test_data_args['training']=False\n","test_data_args['shuffle']=False\n","test_data_args['heldout_split']=0.0\n","\n","    # Now we initialize the test module with the modified config\n","test_dm = datamodule_class(**test_data_args)\n","test_loader = test_dm.get_loader()\n","\n","\n","def evaluate_model(trainer, model_path):\n","    trainer.load_model(model_path)\n","    eval_result = trainer.evaluate(loader=test_loader)\n","    return eval_result\n","\n","\n","# Paths to the saved models\n","best_model_path_aug1 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Q3A_AUG1/best_val_model.pth'\n","last_model_path_aug1 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Q3A_AUG1/last_model.pth'\n","\n","best_model_path_aug2 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Q3A_AUG2/best_val_model.pth'\n","last_model_path_aug2 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Q3A_AUG2/last_model.pth'\n","\n","best_model_path_aug3 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Q3A_AUG3/best_val_model.pth'\n","last_model_path_aug3 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Q3A_AUG3/last_model.pth'\n","\n","best_model_path_aug4 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Q3A_AUG4/best_val_model.pth'\n","last_model_path_aug4 = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Q3A_AUG4/last_model.pth'\n","\n","# Evaluate each model\n","eval_best_aug1 = evaluate_model(trainer_aug1, best_model_path_aug1)\n","eval_last_aug1 = evaluate_model(trainer_aug1, last_model_path_aug1)\n","\n","eval_best_aug2 = evaluate_model(trainer_aug2, best_model_path_aug2)\n","eval_last_aug2 = evaluate_model(trainer_aug2, last_model_path_aug2)\n","\n","eval_best_aug3 = evaluate_model(trainer_aug3, best_model_path_aug3)\n","eval_last_aug3 = evaluate_model(trainer_aug3, last_model_path_aug3)\n","\n","eval_best_aug4 = evaluate_model(trainer_aug4, best_model_path_aug4)\n","eval_last_aug4 = evaluate_model(trainer_aug4, last_model_path_aug4)\n","\n","# Print evaluation results\n","print(\"Augmentation 1 - Best Model Evaluation:\", eval_best_aug1)\n","print(\"Augmentation 1 - Last Model Evaluation:\", eval_last_aug1)\n","print(\"Augmentation 2 - Best Model Evaluation:\", eval_best_aug2)\n","print(\"Augmentation 2 - Last Model Evaluation:\", eval_last_aug2)\n","print(\"Augmentation 3 - Best Model Evaluation:\", eval_best_aug3)\n","print(\"Augmentation 3 - Last Model Evaluation:\", eval_last_aug3)\n","print(\"Augmentation 4 - Best Model Evaluation:\", eval_best_aug4)\n","print(\"Augmentation 4 - Last Model Evaluation:\", eval_last_aug4)"],"metadata":{"id":"wCHkdOMNJT58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718043368627,"user_tz":-120,"elapsed":153043,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"22542778-c26f-45a9-9fd9-6824d2709f3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10 for split eval are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 10000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.4649: : 100% 10000/10000 [00:25<00:00, 385.19it/s]\n","Eval Loss: 0.6133: : 100% 10000/10000 [00:20<00:00, 487.76it/s]\n","Eval Loss: 0.5232: : 100% 10000/10000 [00:15<00:00, 625.49it/s]\n","Eval Loss: 0.5232: : 100% 10000/10000 [00:17<00:00, 560.33it/s]\n","Eval Loss: 0.6150: : 100% 10000/10000 [00:16<00:00, 609.17it/s]\n","Eval Loss: 0.6971: : 100% 10000/10000 [00:18<00:00, 551.40it/s]\n","Eval Loss: 0.5725: : 100% 10000/10000 [00:15<00:00, 639.98it/s]\n","Eval Loss: 0.6894: : 100% 10000/10000 [00:18<00:00, 539.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Augmentation 1 - Best Model Evaluation: {'loss': 0.5661794936656952, 'top1': 0.8368000000000001, 'top5': 0.9924999999999998}\n","Augmentation 1 - Last Model Evaluation: {'loss': 0.6962372756004334, 'top1': 0.8108999999999997, 'top5': 0.9887999999999999}\n","Augmentation 2 - Best Model Evaluation: {'loss': 0.46157553493976594, 'top1': 0.8514999999999998, 'top5': 0.9938999999999997}\n","Augmentation 2 - Last Model Evaluation: {'loss': 0.46157553493976594, 'top1': 0.8514999999999998, 'top5': 0.9938999999999997}\n","Augmentation 3 - Best Model Evaluation: {'loss': 0.5484165585041046, 'top1': 0.8301, 'top5': 0.9913}\n","Augmentation 3 - Last Model Evaluation: {'loss': 0.5747123038768769, 'top1': 0.8289000000000002, 'top5': 0.9887999999999998}\n","Augmentation 4 - Best Model Evaluation: {'loss': 0.4600054389238358, 'top1': 0.8460000000000001, 'top5': 0.9937999999999998}\n","Augmentation 4 - Last Model Evaluation: {'loss': 0.5083837759494781, 'top1': 0.8342999999999996, 'top5': 0.9919999999999998}\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# b\n","import pickle\n","import torch\n","import matplotlib.pyplot as plt\n","from cfgs.exercise_3 import cnn_cifar10\n","\n","# Load the dropout_experiments dictionary\n","with open(\"/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Dropout/dropout_experiments.pkl\", \"rb\") as f:\n","    dropout_experiments = pickle.load(f)\n","\n","# Function to create and run the trainer\n","# def create_trainer(config):\n","#     datamodule_class = config['datamodule']\n","#     data_args = config['data_args']\n","\n","#     dm = datamodule_class(**data_args)\n","\n","#     train_data_loader = dm.get_loader()\n","#     valid_data_loader = dm.get_heldout_loader()\n","\n","#     trainer_class = config['trainer_module']\n","#     trainer = trainer_class(\n","#         config=config,\n","#         log_dir='/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs',\n","#         train_loader=train_data_loader,\n","#         eval_loader=valid_data_loader,\n","#     )\n","\n","\n","for p, config in dropout_experiments.items():\n","    print(f\"Running experiment with dropout={p}\")\n","    datamodule_class = config['datamodule']\n","    data_args = config['data_args']\n","\n","    dm = datamodule_class(**data_args)\n","\n","    train_data_loader = dm.get_loader()\n","    valid_data_loader = dm.get_heldout_loader()\n","\n","    trainer_class = config['trainer_module']\n","    trainer = trainer_class(\n","        config=config,\n","        log_dir='/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs',\n","        train_loader=train_data_loader,\n","        eval_loader=valid_data_loader,\n","    )\n","    trainer.train()\n"],"metadata":{"id":"C2YnISclKRi1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"22e1a762-f6a0-4641-b168-5e22e054ee82","executionInfo":{"status":"ok","timestamp":1718123086685,"user_tz":-120,"elapsed":293163,"user":{"displayName":"Parul Negi","userId":"14413694775804507286"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Running experiment with dropout=dropout_0.1\n","transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","Warning! Log file /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs/CIFAR10_CNN_Dropout_Experiment_dropout_0.1.log already exists! The logs will be appended!\n","Warning! Save dir /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Dropout_Experiment_dropout_0.1 already exists!Existing checkpoints will be overwritten!\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","  0% 0/45000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Train Epoch: 1 Loss: 0.9831: : 100% 45000/45000 [00:26<00:00, 1677.52it/s]\n","Eval Loss: 1.0468: : 100% 5000/5000 [00:08<00:00, 577.30it/s]\n","Train Epoch: 2 Loss: 0.8663: : 100% 45000/45000 [00:23<00:00, 1951.46it/s]\n","Eval Loss: 1.1072: : 100% 5000/5000 [00:07<00:00, 703.16it/s]\n","Train Epoch: 3 Loss: 0.8785: : 100% 45000/45000 [00:24<00:00, 1853.59it/s]\n","Eval Loss: 0.8891: : 100% 5000/5000 [00:09<00:00, 527.22it/s]\n","Train Epoch: 4 Loss: 0.7997: : 100% 45000/45000 [00:24<00:00, 1852.62it/s]\n","Eval Loss: 0.8986: : 100% 5000/5000 [00:10<00:00, 475.28it/s]\n","Train Epoch: 5 Loss: 0.7644: : 100% 45000/45000 [00:25<00:00, 1738.93it/s]\n","Eval Loss: 0.7097: : 100% 5000/5000 [00:08<00:00, 575.52it/s]\n","Train Epoch: 6 Loss: 0.5601: : 100% 45000/45000 [00:25<00:00, 1798.15it/s]\n","Eval Loss: 0.6784: : 100% 5000/5000 [00:06<00:00, 766.61it/s]\n","Train Epoch: 7 Loss: 0.5661: : 100% 45000/45000 [00:21<00:00, 2076.93it/s]\n","Eval Loss: 0.5674: : 100% 5000/5000 [00:10<00:00, 476.06it/s]\n","Train Epoch: 8 Loss: 0.4474: : 100% 45000/45000 [00:21<00:00, 2131.90it/s]\n","Eval Loss: 0.6017: : 100% 5000/5000 [00:08<00:00, 609.31it/s]\n","Train Epoch: 9 Loss: 0.5208: : 100% 45000/45000 [00:23<00:00, 1910.05it/s]\n","Eval Loss: 0.6460: : 100% 5000/5000 [00:07<00:00, 640.17it/s]\n","Train Epoch: 10 Loss: 0.4210: : 100% 45000/45000 [00:22<00:00, 1974.02it/s]\n","Eval Loss: 0.7403: : 100% 5000/5000 [00:09<00:00, 532.52it/s]\n","Train Epoch: 11 Loss: 0.4909: : 100% 45000/45000 [00:21<00:00, 2084.29it/s]\n","Eval Loss: 0.6898: : 100% 5000/5000 [00:06<00:00, 724.04it/s]\n","Train Epoch: 12 Loss: 0.4047: : 100% 45000/45000 [00:24<00:00, 1851.54it/s]\n","Eval Loss: 0.5699: : 100% 5000/5000 [00:09<00:00, 542.53it/s]\n","Train Epoch: 13 Loss: 0.5077: : 100% 45000/45000 [00:22<00:00, 2033.41it/s]\n","Eval Loss: 0.5299: : 100% 5000/5000 [00:08<00:00, 583.07it/s]\n","Train Epoch: 14 Loss: 0.4406: : 100% 45000/45000 [00:21<00:00, 2090.02it/s]\n","Eval Loss: 0.4452: : 100% 5000/5000 [00:06<00:00, 750.95it/s]\n","Train Epoch: 15 Loss: 0.3724: : 100% 45000/45000 [00:22<00:00, 1965.79it/s]\n","Eval Loss: 0.4899: : 100% 5000/5000 [00:09<00:00, 518.93it/s]\n","Train Epoch: 16 Loss: 0.3318: : 100% 45000/45000 [00:21<00:00, 2107.51it/s]\n","Eval Loss: 0.4495: : 100% 5000/5000 [00:06<00:00, 734.96it/s]\n","Train Epoch: 17 Loss: 0.2509: : 100% 45000/45000 [00:27<00:00, 1608.97it/s]\n","Eval Loss: 0.4551: : 100% 5000/5000 [00:07<00:00, 630.37it/s]\n","Train Epoch: 18 Loss: 0.2512: : 100% 45000/45000 [00:24<00:00, 1867.12it/s]\n","Eval Loss: 0.4502: : 100% 5000/5000 [00:10<00:00, 466.87it/s]\n","Train Epoch: 19 Loss: 0.3806: : 100% 45000/45000 [00:24<00:00, 1857.70it/s]\n","Eval Loss: 0.5097: : 100% 5000/5000 [00:09<00:00, 554.53it/s]\n","Train Epoch: 20 Loss: 0.2818: : 100% 45000/45000 [00:23<00:00, 1886.23it/s]\n","Eval Loss: 0.4705: : 100% 5000/5000 [00:10<00:00, 481.31it/s]\n","Train Epoch: 21 Loss: 0.2493: : 100% 45000/45000 [00:22<00:00, 1971.52it/s]\n","Eval Loss: 0.4311: : 100% 5000/5000 [00:07<00:00, 638.78it/s]\n","Train Epoch: 22 Loss: 0.1650: : 100% 45000/45000 [00:24<00:00, 1862.77it/s]\n","Eval Loss: 0.4372: : 100% 5000/5000 [00:07<00:00, 672.45it/s]\n","Train Epoch: 23 Loss: 0.2221: : 100% 45000/45000 [00:23<00:00, 1912.43it/s]\n","Eval Loss: 0.5022: : 100% 5000/5000 [00:11<00:00, 428.30it/s]\n","Train Epoch: 24 Loss: 0.1657: : 100% 45000/45000 [00:22<00:00, 2016.41it/s]\n","Eval Loss: 0.5128: : 100% 5000/5000 [00:08<00:00, 600.76it/s]\n","Train Epoch: 25 Loss: 0.1982: : 100% 45000/45000 [00:24<00:00, 1830.10it/s]\n","Eval Loss: 0.5125: : 100% 5000/5000 [00:06<00:00, 770.19it/s]\n","Train Epoch: 26 Loss: 0.1308: : 100% 45000/45000 [00:20<00:00, 2206.12it/s]\n","Eval Loss: 0.4118: : 100% 5000/5000 [00:08<00:00, 619.92it/s]\n","Train Epoch: 27 Loss: 0.1382: : 100% 45000/45000 [00:20<00:00, 2152.08it/s]\n","Eval Loss: 0.4254: : 100% 5000/5000 [00:06<00:00, 766.92it/s]\n","Train Epoch: 28 Loss: 0.0809: : 100% 45000/45000 [00:20<00:00, 2209.82it/s]\n","Eval Loss: 0.5545: : 100% 5000/5000 [00:09<00:00, 551.81it/s]\n","Train Epoch: 29 Loss: 0.1422: : 100% 45000/45000 [00:24<00:00, 1832.59it/s]\n","Eval Loss: 0.4465: : 100% 5000/5000 [00:06<00:00, 723.40it/s]\n","Train Epoch: 30 Loss: 0.1445: : 100% 45000/45000 [00:20<00:00, 2192.38it/s]\n","Eval Loss: 0.5225: : 100% 5000/5000 [00:09<00:00, 537.31it/s]\n","Eval Loss: 0.5225: : 100% 5000/5000 [00:06<00:00, 781.11it/s]\n","Eval Loss: 0.5225: : 100% 5000/5000 [00:08<00:00, 579.82it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Running experiment with dropout=dropout_0.3\n","transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Train Epoch: 1 Loss: 1.0660: : 100% 45000/45000 [00:23<00:00, 1939.54it/s]\n","Eval Loss: 1.0952: : 100% 5000/5000 [00:08<00:00, 578.89it/s]\n","Train Epoch: 2 Loss: 1.0131: : 100% 45000/45000 [00:26<00:00, 1680.97it/s]\n","Eval Loss: 1.1901: : 100% 5000/5000 [00:06<00:00, 727.42it/s]\n","Train Epoch: 3 Loss: 0.8575: : 100% 45000/45000 [00:21<00:00, 2073.71it/s]\n","Eval Loss: 1.0723: : 100% 5000/5000 [00:11<00:00, 442.51it/s]\n","Train Epoch: 4 Loss: 0.9038: : 100% 45000/45000 [00:23<00:00, 1936.68it/s]\n","Eval Loss: 0.8416: : 100% 5000/5000 [00:06<00:00, 750.62it/s]\n","Train Epoch: 5 Loss: 0.7524: : 100% 45000/45000 [00:23<00:00, 1926.81it/s]\n","Eval Loss: 0.7936: : 100% 5000/5000 [00:06<00:00, 753.83it/s]\n","Train Epoch: 6 Loss: 0.8316: : 100% 45000/45000 [00:22<00:00, 2024.76it/s]\n","Eval Loss: 0.7224: : 100% 5000/5000 [00:08<00:00, 588.86it/s]\n","Train Epoch: 7 Loss: 0.7758: : 100% 45000/45000 [00:21<00:00, 2071.73it/s]\n","Eval Loss: 0.6621: : 100% 5000/5000 [00:06<00:00, 775.69it/s]\n","Train Epoch: 8 Loss: 0.7819: : 100% 45000/45000 [00:24<00:00, 1866.03it/s]\n","Eval Loss: 0.5602: : 100% 5000/5000 [00:08<00:00, 564.05it/s]\n","Train Epoch: 9 Loss: 0.5758: : 100% 45000/45000 [00:21<00:00, 2130.14it/s]\n","Eval Loss: 0.6415: : 100% 5000/5000 [00:06<00:00, 793.84it/s]\n","Train Epoch: 10 Loss: 0.6474: : 100% 45000/45000 [00:21<00:00, 2128.79it/s]\n","Eval Loss: 0.5813: : 100% 5000/5000 [00:09<00:00, 554.88it/s]\n","Train Epoch: 11 Loss: 0.7108: : 100% 45000/45000 [00:20<00:00, 2144.99it/s]\n","Eval Loss: 0.5210: : 100% 5000/5000 [00:06<00:00, 785.08it/s]\n","Train Epoch: 12 Loss: 0.6040: : 100% 45000/45000 [00:21<00:00, 2120.58it/s]\n","Eval Loss: 0.6400: : 100% 5000/5000 [00:08<00:00, 564.16it/s]\n","Train Epoch: 13 Loss: 0.5148: : 100% 45000/45000 [00:21<00:00, 2120.95it/s]\n","Eval Loss: 0.6135: : 100% 5000/5000 [00:07<00:00, 632.89it/s]\n","Train Epoch: 14 Loss: 0.6489: : 100% 45000/45000 [00:22<00:00, 1980.69it/s]\n","Eval Loss: 0.5403: : 100% 5000/5000 [00:07<00:00, 706.00it/s]\n","Train Epoch: 15 Loss: 0.6234: : 100% 45000/45000 [00:22<00:00, 2006.57it/s]\n","Eval Loss: 0.5749: : 100% 5000/5000 [00:09<00:00, 535.28it/s]\n","Train Epoch: 16 Loss: 0.4754: : 100% 45000/45000 [00:23<00:00, 1948.46it/s]\n","Eval Loss: 0.4625: : 100% 5000/5000 [00:07<00:00, 637.81it/s]\n","Train Epoch: 17 Loss: 0.5274: : 100% 45000/45000 [00:23<00:00, 1947.80it/s]\n","Eval Loss: 0.6131: : 100% 5000/5000 [00:10<00:00, 487.35it/s]\n","Train Epoch: 18 Loss: 0.6323: : 100% 45000/45000 [00:23<00:00, 1883.17it/s]\n","Eval Loss: 0.4948: : 100% 5000/5000 [00:08<00:00, 617.22it/s]\n","Train Epoch: 19 Loss: 0.5082: : 100% 45000/45000 [00:24<00:00, 1873.43it/s]\n","Eval Loss: 0.4752: : 100% 5000/5000 [00:06<00:00, 763.67it/s]\n","Train Epoch: 20 Loss: 0.8199: : 100% 45000/45000 [00:21<00:00, 2065.71it/s]\n","Eval Loss: 0.4891: : 100% 5000/5000 [00:09<00:00, 551.91it/s]\n","Train Epoch: 21 Loss: 0.4396: : 100% 45000/45000 [00:23<00:00, 1916.80it/s]\n","Eval Loss: 0.4331: : 100% 5000/5000 [00:08<00:00, 582.84it/s]\n","Train Epoch: 22 Loss: 0.4886: : 100% 45000/45000 [00:22<00:00, 2034.98it/s]\n","Eval Loss: 0.4850: : 100% 5000/5000 [00:06<00:00, 777.22it/s]\n","Train Epoch: 23 Loss: 0.3945: : 100% 45000/45000 [00:22<00:00, 1979.77it/s]\n","Eval Loss: 0.4540: : 100% 5000/5000 [00:08<00:00, 570.15it/s]\n","Train Epoch: 24 Loss: 0.4064: : 100% 45000/45000 [00:21<00:00, 2133.43it/s]\n","Eval Loss: 0.4615: : 100% 5000/5000 [00:07<00:00, 648.72it/s]\n","Train Epoch: 25 Loss: 0.4114: : 100% 45000/45000 [00:22<00:00, 1980.58it/s]\n","Eval Loss: 0.4763: : 100% 5000/5000 [00:06<00:00, 742.16it/s]\n","Train Epoch: 26 Loss: 0.3256: : 100% 45000/45000 [00:21<00:00, 2068.14it/s]\n","Eval Loss: 0.4759: : 100% 5000/5000 [00:08<00:00, 570.32it/s]\n","Train Epoch: 27 Loss: 0.3654: : 100% 45000/45000 [00:21<00:00, 2053.57it/s]\n","Eval Loss: 0.3972: : 100% 5000/5000 [00:06<00:00, 785.66it/s]\n","Train Epoch: 28 Loss: 0.2706: : 100% 45000/45000 [00:24<00:00, 1866.43it/s]\n","Eval Loss: 0.4403: : 100% 5000/5000 [00:10<00:00, 495.62it/s]\n","Train Epoch: 29 Loss: 0.3553: : 100% 45000/45000 [00:22<00:00, 2012.72it/s]\n","Eval Loss: 0.3798: : 100% 5000/5000 [00:06<00:00, 797.55it/s]\n","Train Epoch: 30 Loss: 0.2861: : 100% 45000/45000 [00:20<00:00, 2200.83it/s]\n","Eval Loss: 0.4103: : 100% 5000/5000 [00:09<00:00, 555.55it/s]\n","Eval Loss: 0.4103: : 100% 5000/5000 [00:06<00:00, 787.44it/s]\n","Eval Loss: 0.4103: : 100% 5000/5000 [00:09<00:00, 530.94it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Running experiment with dropout=dropout_0.5\n","transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Train Epoch: 1 Loss: 1.2434: : 100% 45000/45000 [00:20<00:00, 2195.06it/s]\n","Eval Loss: 1.5250: : 100% 5000/5000 [00:06<00:00, 781.95it/s]\n","Train Epoch: 2 Loss: 1.0345: : 100% 45000/45000 [00:22<00:00, 2042.06it/s]\n","Eval Loss: 1.2268: : 100% 5000/5000 [00:09<00:00, 512.17it/s]\n","Train Epoch: 3 Loss: 1.0422: : 100% 45000/45000 [00:22<00:00, 2039.88it/s]\n","Eval Loss: 0.9151: : 100% 5000/5000 [00:06<00:00, 718.45it/s]\n","Train Epoch: 4 Loss: 1.0259: : 100% 45000/45000 [00:22<00:00, 2043.79it/s]\n","Eval Loss: 0.8809: : 100% 5000/5000 [00:06<00:00, 762.74it/s]\n","Train Epoch: 5 Loss: 1.0810: : 100% 45000/45000 [00:21<00:00, 2096.61it/s]\n","Eval Loss: 0.9274: : 100% 5000/5000 [00:08<00:00, 572.69it/s]\n","Train Epoch: 6 Loss: 0.8592: : 100% 45000/45000 [00:20<00:00, 2176.72it/s]\n","Eval Loss: 0.8634: : 100% 5000/5000 [00:06<00:00, 788.21it/s]\n","Train Epoch: 7 Loss: 0.8047: : 100% 45000/45000 [00:21<00:00, 2084.48it/s]\n","Eval Loss: 0.8344: : 100% 5000/5000 [00:08<00:00, 556.24it/s]\n","Train Epoch: 8 Loss: 0.8346: : 100% 45000/45000 [00:22<00:00, 2016.66it/s]\n","Eval Loss: 0.8284: : 100% 5000/5000 [00:06<00:00, 745.28it/s]\n","Train Epoch: 9 Loss: 0.9022: : 100% 45000/45000 [00:21<00:00, 2068.74it/s]\n","Eval Loss: 0.7200: : 100% 5000/5000 [00:06<00:00, 759.20it/s]\n","Train Epoch: 10 Loss: 0.8036: : 100% 45000/45000 [00:20<00:00, 2157.47it/s]\n","Eval Loss: 0.7711: : 100% 5000/5000 [00:07<00:00, 631.16it/s]\n","Train Epoch: 11 Loss: 0.8113: : 100% 45000/45000 [00:20<00:00, 2153.56it/s]\n","Eval Loss: 0.6485: : 100% 5000/5000 [00:06<00:00, 794.10it/s]\n","Train Epoch: 12 Loss: 0.8522: : 100% 45000/45000 [00:21<00:00, 2116.32it/s]\n","Eval Loss: 0.7400: : 100% 5000/5000 [00:08<00:00, 565.33it/s]\n","Train Epoch: 13 Loss: 0.8144: : 100% 45000/45000 [00:20<00:00, 2173.57it/s]\n","Eval Loss: 0.7734: : 100% 5000/5000 [00:06<00:00, 790.23it/s]\n","Train Epoch: 14 Loss: 0.6927: : 100% 45000/45000 [00:20<00:00, 2176.31it/s]\n","Eval Loss: 0.6727: : 100% 5000/5000 [00:08<00:00, 577.69it/s]\n","Train Epoch: 15 Loss: 0.8080: : 100% 45000/45000 [00:20<00:00, 2170.24it/s]\n","Eval Loss: 0.6036: : 100% 5000/5000 [00:06<00:00, 798.90it/s]\n","Train Epoch: 16 Loss: 0.6867: : 100% 45000/45000 [00:20<00:00, 2164.07it/s]\n","Eval Loss: 0.6503: : 100% 5000/5000 [00:08<00:00, 558.83it/s]\n","Train Epoch: 17 Loss: 0.8066: : 100% 45000/45000 [00:20<00:00, 2161.46it/s]\n","Eval Loss: 0.6274: : 100% 5000/5000 [00:06<00:00, 761.64it/s]\n","Train Epoch: 18 Loss: 0.6074: : 100% 45000/45000 [00:23<00:00, 1926.03it/s]\n","Eval Loss: 0.5887: : 100% 5000/5000 [00:07<00:00, 691.09it/s]\n","Train Epoch: 19 Loss: 0.5565: : 100% 45000/45000 [00:22<00:00, 1995.99it/s]\n","Eval Loss: 0.5480: : 100% 5000/5000 [00:07<00:00, 691.23it/s]\n","Train Epoch: 20 Loss: 0.6622: : 100% 45000/45000 [00:21<00:00, 2120.47it/s]\n","Eval Loss: 0.5959: : 100% 5000/5000 [00:08<00:00, 580.79it/s]\n","Train Epoch: 21 Loss: 0.6722: : 100% 45000/45000 [00:21<00:00, 2075.16it/s]\n","Eval Loss: 0.5977: : 100% 5000/5000 [00:06<00:00, 768.16it/s]\n","Train Epoch: 22 Loss: 0.6933: : 100% 45000/45000 [00:20<00:00, 2158.61it/s]\n","Eval Loss: 0.5360: : 100% 5000/5000 [00:08<00:00, 591.19it/s]\n","Train Epoch: 23 Loss: 0.6665: : 100% 45000/45000 [00:20<00:00, 2203.19it/s]\n","Eval Loss: 0.5301: : 100% 5000/5000 [00:06<00:00, 785.69it/s]\n","Train Epoch: 24 Loss: 0.6736: : 100% 45000/45000 [00:20<00:00, 2154.41it/s]\n","Eval Loss: 0.5431: : 100% 5000/5000 [00:08<00:00, 572.40it/s]\n","Train Epoch: 25 Loss: 0.7200: : 100% 45000/45000 [00:20<00:00, 2192.97it/s]\n","Eval Loss: 0.5166: : 100% 5000/5000 [00:06<00:00, 762.19it/s]\n","Train Epoch: 26 Loss: 0.4524: : 100% 45000/45000 [00:21<00:00, 2098.50it/s]\n","Eval Loss: 0.5209: : 100% 5000/5000 [00:08<00:00, 573.40it/s]\n","Train Epoch: 27 Loss: 0.4007: : 100% 45000/45000 [00:20<00:00, 2145.26it/s]\n","Eval Loss: 0.4599: : 100% 5000/5000 [00:06<00:00, 780.20it/s]\n","Train Epoch: 28 Loss: 0.5594: : 100% 45000/45000 [00:23<00:00, 1910.81it/s]\n","Eval Loss: 0.4989: : 100% 5000/5000 [00:08<00:00, 580.92it/s]\n","Train Epoch: 29 Loss: 0.6003: : 100% 45000/45000 [00:21<00:00, 2132.98it/s]\n","Eval Loss: 0.4796: : 100% 5000/5000 [00:06<00:00, 791.32it/s]\n","Train Epoch: 30 Loss: 0.5445: : 100% 45000/45000 [00:20<00:00, 2163.03it/s]\n","Eval Loss: 0.4858: : 100% 5000/5000 [00:08<00:00, 570.26it/s]\n","Eval Loss: 0.4858: : 100% 5000/5000 [00:06<00:00, 768.69it/s]\n","Eval Loss: 0.4858: : 100% 5000/5000 [00:07<00:00, 634.09it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Running experiment with dropout=dropout_0.7\n","transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Train Epoch: 1 Loss: 1.4542: : 100% 45000/45000 [00:21<00:00, 2131.80it/s]\n","Eval Loss: 2.0779: : 100% 5000/5000 [00:06<00:00, 781.68it/s]\n","Train Epoch: 2 Loss: 1.2706: : 100% 45000/45000 [00:23<00:00, 1948.31it/s]\n","Eval Loss: 1.7575: : 100% 5000/5000 [00:08<00:00, 560.11it/s]\n","Train Epoch: 3 Loss: 1.2878: : 100% 45000/45000 [00:21<00:00, 2124.30it/s]\n","Eval Loss: 1.4969: : 100% 5000/5000 [00:06<00:00, 783.77it/s]\n","Train Epoch: 4 Loss: 1.2488: : 100% 45000/45000 [00:22<00:00, 1965.75it/s]\n","Eval Loss: 1.2307: : 100% 5000/5000 [00:08<00:00, 577.09it/s]\n","Train Epoch: 5 Loss: 1.2577: : 100% 45000/45000 [00:20<00:00, 2149.75it/s]\n","Eval Loss: 1.1102: : 100% 5000/5000 [00:06<00:00, 740.00it/s]\n","Train Epoch: 6 Loss: 1.2271: : 100% 45000/45000 [00:21<00:00, 2064.24it/s]\n","Eval Loss: 1.1962: : 100% 5000/5000 [00:06<00:00, 754.84it/s]\n","Train Epoch: 7 Loss: 1.1505: : 100% 45000/45000 [00:22<00:00, 1989.19it/s]\n","Eval Loss: 1.3402: : 100% 5000/5000 [00:08<00:00, 574.51it/s]\n","Train Epoch: 8 Loss: 1.0383: : 100% 45000/45000 [00:20<00:00, 2157.71it/s]\n","Eval Loss: 0.9598: : 100% 5000/5000 [00:06<00:00, 782.57it/s]\n","Train Epoch: 9 Loss: 1.0415: : 100% 45000/45000 [00:21<00:00, 2089.45it/s]\n","Eval Loss: 1.0512: : 100% 5000/5000 [00:08<00:00, 565.02it/s]\n","Train Epoch: 10 Loss: 1.2070: : 100% 45000/45000 [00:20<00:00, 2158.15it/s]\n","Eval Loss: 1.0957: : 100% 5000/5000 [00:06<00:00, 790.08it/s]\n","Train Epoch: 11 Loss: 1.0627: : 100% 45000/45000 [00:21<00:00, 2061.50it/s]\n","Eval Loss: 1.0072: : 100% 5000/5000 [00:08<00:00, 598.75it/s]\n","Train Epoch: 12 Loss: 1.0299: : 100% 45000/45000 [00:21<00:00, 2136.40it/s]\n","Eval Loss: 1.0532: : 100% 5000/5000 [00:06<00:00, 774.19it/s]\n","Train Epoch: 13 Loss: 1.1371: : 100% 45000/45000 [00:21<00:00, 2069.58it/s]\n","Eval Loss: 1.0887: : 100% 5000/5000 [00:08<00:00, 592.83it/s]\n","Train Epoch: 14 Loss: 1.1236: : 100% 45000/45000 [00:20<00:00, 2164.65it/s]\n","Eval Loss: 0.9819: : 100% 5000/5000 [00:06<00:00, 735.19it/s]\n","Train Epoch: 15 Loss: 1.0767: : 100% 45000/45000 [00:21<00:00, 2105.44it/s]\n","Eval Loss: 0.9361: : 100% 5000/5000 [00:06<00:00, 768.10it/s]\n","Train Epoch: 16 Loss: 0.8913: : 100% 45000/45000 [00:20<00:00, 2188.95it/s]\n","Eval Loss: 0.8839: : 100% 5000/5000 [00:08<00:00, 578.30it/s]\n","Train Epoch: 17 Loss: 0.9319: : 100% 45000/45000 [00:21<00:00, 2101.97it/s]\n","Eval Loss: 0.9603: : 100% 5000/5000 [00:08<00:00, 575.87it/s]\n","Train Epoch: 18 Loss: 0.9174: : 100% 45000/45000 [00:21<00:00, 2104.34it/s]\n","Eval Loss: 0.9707: : 100% 5000/5000 [00:08<00:00, 565.35it/s]\n","Train Epoch: 19 Loss: 0.9061: : 100% 45000/45000 [00:20<00:00, 2149.36it/s]\n","Eval Loss: 0.8627: : 100% 5000/5000 [00:06<00:00, 754.02it/s]\n","Train Epoch: 20 Loss: 0.9304: : 100% 45000/45000 [00:21<00:00, 2061.03it/s]\n","Eval Loss: 0.8481: : 100% 5000/5000 [00:08<00:00, 564.21it/s]\n","Train Epoch: 21 Loss: 0.9276: : 100% 45000/45000 [00:21<00:00, 2074.18it/s]\n","Eval Loss: 0.8376: : 100% 5000/5000 [00:06<00:00, 736.08it/s]\n","Train Epoch: 22 Loss: 0.8859: : 100% 45000/45000 [00:22<00:00, 2006.11it/s]\n","Eval Loss: 0.9021: : 100% 5000/5000 [00:06<00:00, 727.53it/s]\n","Train Epoch: 23 Loss: 0.8888: : 100% 45000/45000 [00:21<00:00, 2085.81it/s]\n","Eval Loss: 0.9580: : 100% 5000/5000 [00:08<00:00, 567.29it/s]\n","Train Epoch: 24 Loss: 0.9506: : 100% 45000/45000 [00:20<00:00, 2153.52it/s]\n","Eval Loss: 0.8135: : 100% 5000/5000 [00:06<00:00, 782.46it/s]\n","Train Epoch: 25 Loss: 0.7506: : 100% 45000/45000 [00:21<00:00, 2083.08it/s]\n","Eval Loss: 0.8380: : 100% 5000/5000 [00:09<00:00, 551.81it/s]\n","Train Epoch: 26 Loss: 0.8286: : 100% 45000/45000 [00:21<00:00, 2057.70it/s]\n","Eval Loss: 0.8260: : 100% 5000/5000 [00:06<00:00, 754.56it/s]\n","Train Epoch: 27 Loss: 0.8309: : 100% 45000/45000 [00:23<00:00, 1953.37it/s]\n","Eval Loss: 0.7297: : 100% 5000/5000 [00:09<00:00, 545.02it/s]\n","Train Epoch: 28 Loss: 0.9908: : 100% 45000/45000 [00:22<00:00, 2018.57it/s]\n","Eval Loss: 0.8470: : 100% 5000/5000 [00:06<00:00, 750.90it/s]\n","Train Epoch: 29 Loss: 0.8093: : 100% 45000/45000 [00:22<00:00, 2019.13it/s]\n","Eval Loss: 0.7343: : 100% 5000/5000 [00:08<00:00, 557.55it/s]\n","Train Epoch: 30 Loss: 0.8118: : 100% 45000/45000 [00:21<00:00, 2069.73it/s]\n","Eval Loss: 0.7961: : 100% 5000/5000 [00:06<00:00, 718.68it/s]\n","Eval Loss: 0.7961: : 100% 5000/5000 [00:09<00:00, 522.98it/s]\n","Eval Loss: 0.7961: : 100% 5000/5000 [00:06<00:00, 760.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Running experiment with dropout=dropout_0.9\n","transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: 1 Loss: 2.0922: : 100% 45000/45000 [00:21<00:00, 2068.58it/s]\n","Eval Loss: 3.0238: : 100% 5000/5000 [00:07<00:00, 657.33it/s]\n","Train Epoch: 2 Loss: 1.7982: : 100% 45000/45000 [00:22<00:00, 2044.56it/s]\n","Eval Loss: 3.0818: : 100% 5000/5000 [00:09<00:00, 530.57it/s]\n","Train Epoch: 3 Loss: 1.8510: : 100% 45000/45000 [00:23<00:00, 1926.54it/s]\n","Eval Loss: 3.1314: : 100% 5000/5000 [00:06<00:00, 735.68it/s]\n","Train Epoch: 4 Loss: 1.7314: : 100% 45000/45000 [00:23<00:00, 1940.99it/s]\n","Eval Loss: 3.0523: : 100% 5000/5000 [00:08<00:00, 556.04it/s]\n","Train Epoch: 5 Loss: 1.7963: : 100% 45000/45000 [00:22<00:00, 2042.91it/s]\n","Eval Loss: 3.0302: : 100% 5000/5000 [00:07<00:00, 710.70it/s]\n","Train Epoch: 6 Loss: 1.7494: : 100% 45000/45000 [00:24<00:00, 1819.18it/s]\n","Eval Loss: 3.0723: : 100% 5000/5000 [00:06<00:00, 739.14it/s]\n","Train Epoch: 7 Loss: 1.7108: : 100% 45000/45000 [00:21<00:00, 2117.13it/s]\n","Eval Loss: 3.0483: : 100% 5000/5000 [00:08<00:00, 570.59it/s]\n","Train Epoch: 8 Loss: 1.7397: : 100% 45000/45000 [00:21<00:00, 2092.81it/s]\n","Eval Loss: 3.0528: : 100% 5000/5000 [00:06<00:00, 766.73it/s]\n","Train Epoch: 9 Loss: 1.7603: : 100% 45000/45000 [00:21<00:00, 2124.67it/s]\n","Eval Loss: 2.9925: : 100% 5000/5000 [00:08<00:00, 561.06it/s]\n","Train Epoch: 10 Loss: 1.8305: : 100% 45000/45000 [00:21<00:00, 2108.97it/s]\n","Eval Loss: 2.9260: : 100% 5000/5000 [00:06<00:00, 756.32it/s]\n","Train Epoch: 11 Loss: 1.7408: : 100% 45000/45000 [00:22<00:00, 2043.16it/s]\n","Eval Loss: 2.8467: : 100% 5000/5000 [00:08<00:00, 573.56it/s]\n","Train Epoch: 12 Loss: 1.8033: : 100% 45000/45000 [00:21<00:00, 2109.17it/s]\n","Eval Loss: 2.8895: : 100% 5000/5000 [00:07<00:00, 711.83it/s]\n","Train Epoch: 13 Loss: 1.8471: : 100% 45000/45000 [00:22<00:00, 1986.24it/s]\n","Eval Loss: 2.8624: : 100% 5000/5000 [00:06<00:00, 739.92it/s]\n","Train Epoch: 14 Loss: 1.7871: : 100% 45000/45000 [00:21<00:00, 2083.98it/s]\n","Eval Loss: 2.8998: : 100% 5000/5000 [00:08<00:00, 614.66it/s]\n","Train Epoch: 15 Loss: 1.6758: : 100% 45000/45000 [00:21<00:00, 2123.46it/s]\n","Eval Loss: 2.9288: : 100% 5000/5000 [00:06<00:00, 755.69it/s]\n","Train Epoch: 16 Loss: 1.6302: : 100% 45000/45000 [00:23<00:00, 1932.18it/s]\n","Eval Loss: 2.9797: : 100% 5000/5000 [00:08<00:00, 609.68it/s]\n","Train Epoch: 17 Loss: 1.6686: : 100% 45000/45000 [00:21<00:00, 2117.21it/s]\n","Eval Loss: 2.9803: : 100% 5000/5000 [00:06<00:00, 778.29it/s]\n","Train Epoch: 18 Loss: 1.7095: : 100% 45000/45000 [00:21<00:00, 2109.78it/s]\n","Eval Loss: 3.0574: : 100% 5000/5000 [00:09<00:00, 531.10it/s]\n","Train Epoch: 19 Loss: 1.6091: : 100% 45000/45000 [00:21<00:00, 2120.34it/s]\n","Eval Loss: 2.9536: : 100% 5000/5000 [00:06<00:00, 768.86it/s]\n","Train Epoch: 20 Loss: 1.6546: : 100% 45000/45000 [00:21<00:00, 2095.82it/s]\n","Eval Loss: 3.0878: : 100% 5000/5000 [00:08<00:00, 567.23it/s]\n","Train Epoch: 21 Loss: 1.8662: : 100% 45000/45000 [00:21<00:00, 2109.83it/s]\n","Eval Loss: 2.9901: : 100% 5000/5000 [00:06<00:00, 780.32it/s]\n","Train Epoch: 22 Loss: 1.6343: : 100% 45000/45000 [00:22<00:00, 2041.36it/s]\n","Eval Loss: 3.1152: : 100% 5000/5000 [00:08<00:00, 575.29it/s]\n","Train Epoch: 23 Loss: 1.6439: : 100% 45000/45000 [00:21<00:00, 2130.80it/s]\n","Eval Loss: 3.0806: : 100% 5000/5000 [00:06<00:00, 754.44it/s]\n","Train Epoch: 24 Loss: 1.6802: : 100% 45000/45000 [00:21<00:00, 2049.52it/s]\n","Eval Loss: 2.9514: : 100% 5000/5000 [00:07<00:00, 667.78it/s]\n","Train Epoch: 25 Loss: 1.6040: : 100% 45000/45000 [00:21<00:00, 2052.27it/s]\n","Eval Loss: 3.1176: : 100% 5000/5000 [00:10<00:00, 495.44it/s]\n","Train Epoch: 26 Loss: 1.6759: : 100% 45000/45000 [00:21<00:00, 2134.41it/s]\n","Eval Loss: 3.1522: : 100% 5000/5000 [00:06<00:00, 770.29it/s]\n","Train Epoch: 27 Loss: 1.5698: : 100% 45000/45000 [00:20<00:00, 2161.30it/s]\n","Eval Loss: 3.2314: : 100% 5000/5000 [00:08<00:00, 576.84it/s]\n","Train Epoch: 28 Loss: 1.5990: : 100% 45000/45000 [00:21<00:00, 2074.05it/s]\n","Eval Loss: 3.1495: : 100% 5000/5000 [00:06<00:00, 754.98it/s]\n","Train Epoch: 29 Loss: 1.6422: : 100% 45000/45000 [00:21<00:00, 2119.62it/s]\n","Eval Loss: 3.1550: : 100% 5000/5000 [00:08<00:00, 558.82it/s]\n","Train Epoch: 30 Loss: 1.5166: : 100% 45000/45000 [00:21<00:00, 2137.60it/s]\n","Eval Loss: 3.1212: : 100% 5000/5000 [00:06<00:00, 791.61it/s]\n","Eval Loss: 3.1212: : 100% 5000/5000 [00:09<00:00, 526.22it/s]\n","Eval Loss: 3.1212: : 100% 5000/5000 [00:06<00:00, 774.52it/s]\n"]}]},{"cell_type":"code","source":["import pickle\n","with open(\"/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Dropout/dropout_experiments.pkl\", \"rb\") as f:\n","    dropout_experiments = pickle.load(f)\n","\n","\n","results = {}\n","for p, config in dropout_experiments.items():\n","    datamodule_class = config['datamodule']\n","    data_args = config['data_args']\n","\n","    dm = datamodule_class(**data_args)\n","\n","    train_data_loader = dm.get_loader()\n","    valid_data_loader = dm.get_heldout_loader()\n","\n","    trainer_class = config['trainer_module']\n","    trainer = trainer_class(\n","        config=config,\n","        log_dir='/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs',\n","        train_loader=train_data_loader,\n","        eval_loader=valid_data_loader,\n","    )\n","  # Load the best model and evaluate\n","    best_model_path = f\"/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/{config['name']}/best_val_model.pth\"\n","    last_model_path = f\"/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/{config['name']}/last_model.pth\"\n","\n","    trainer.load_model(best_model_path)\n","    eval_best = trainer.evaluate(loader=valid_data_loader)\n","    trainer.load_model(last_model_path)\n","    eval_last = trainer.evaluate(loader=valid_data_loader)\n","\n","    results[p] = {\n","        'best': eval_best,\n","        'last': eval_last\n","    }\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NTU9TGd45-ba","executionInfo":{"status":"ok","timestamp":1718141398637,"user_tz":-120,"elapsed":213514,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"83a37027-6bed-4c29-b150-7238ceec3e4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","Warning! Log file /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs/CIFAR10_CNN_Dropout_Experiment_dropout_0.1.log already exists! The logs will be appended!\n","Warning! Save dir /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Dropout_Experiment_dropout_0.1 already exists!Existing checkpoints will be overwritten!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","  0% 0/5000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","Eval Loss: 0.4254: : 100% 5000/5000 [00:13<00:00, 371.86it/s]\n","Eval Loss: 0.5225: : 100% 5000/5000 [00:08<00:00, 601.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","Warning! Log file /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs/CIFAR10_CNN_Dropout_Experiment_dropout_0.3.log already exists! The logs will be appended!\n","Warning! Save dir /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Dropout_Experiment_dropout_0.3 already exists!Existing checkpoints will be overwritten!\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.4103: : 100% 5000/5000 [00:07<00:00, 653.91it/s]\n","Eval Loss: 0.4103: : 100% 5000/5000 [00:10<00:00, 470.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","Warning! Log file /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs/CIFAR10_CNN_Dropout_Experiment_dropout_0.5.log already exists! The logs will be appended!\n","Warning! Save dir /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Dropout_Experiment_dropout_0.5 already exists!Existing checkpoints will be overwritten!\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.4599: : 100% 5000/5000 [00:06<00:00, 786.60it/s]\n","Eval Loss: 0.4858: : 100% 5000/5000 [00:08<00:00, 583.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","Warning! Log file /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs/CIFAR10_CNN_Dropout_Experiment_dropout_0.7.log already exists! The logs will be appended!\n","Warning! Save dir /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Dropout_Experiment_dropout_0.7 already exists!Existing checkpoints will be overwritten!\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.7343: : 100% 5000/5000 [00:08<00:00, 593.23it/s]\n","Eval Loss: 0.7961: : 100% 5000/5000 [00:09<00:00, 500.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10 for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 200, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 200, 'shuffle': False, 'num_workers': 6}\n","Warning! Log file /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Logs/CIFAR10_CNN_Dropout_Experiment_dropout_0.9.log already exists! The logs will be appended!\n","Warning! Save dir /content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_CNN_Dropout_Experiment_dropout_0.9 already exists!Existing checkpoints will be overwritten!\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 3.1314: : 100% 5000/5000 [00:08<00:00, 603.62it/s]\n","Eval Loss: 3.1212: : 100% 5000/5000 [00:06<00:00, 719.21it/s]\n"]}]},{"cell_type":"code","source":["# Extract results for plotting\n","dropout_values = []\n","best_accuracies = []\n","last_accuracies = []\n","\n","for p, result in results.items():\n","    dropout_values.append(p)\n","    best_accuracies.append(result['best']['top1'])\n","    last_accuracies.append(result['last']['top1'])\n","\n","# Plotting the results\n","plt.figure(figsize=(10, 5))\n","plt.plot(dropout_values, best_accuracies, label='Best Model Accuracy', marker='o')\n","plt.plot(dropout_values, last_accuracies, label='Last Model Accuracy', marker='o')\n","plt.xlabel('Dropout Probability')\n","plt.ylabel('Accuracy')\n","plt.title('Effect of Dropout on CIFAR-10 Accuracy')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"JgZMdrQeYgfm","colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"status":"ok","timestamp":1718141488809,"user_tz":-120,"elapsed":25,"user":{"displayName":"Parul Negi","userId":"08425405678186818457"}},"outputId":"d885bcf0-7cd1-4ad5-b6d1-e8373f00cd32"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"atVQF1gnNdBQ"},"source":["Write your report for Q3 in this cell. Feel free to add extra code\n","\n","cells<br>\n","\n","# **Report on Data Augmentation in CNN Training on CIFAR-10 Dataset**\n","\n","**Introduction**:\n","* Data augmentation is a powerful technique used to improve the generalization ability of machine learning models by artificially increasing the size and diversity of the training dataset. In this report, we explore the impact of four different data augmentation strategies on the performance of a CNN trained on the CIFAR-10 dataset. The augmentations include combinations of random horizontal flipping, random rotation, and color jitter.\n","\n","**Data Augmentation Strategies**:\n","* Augmentation 1 (Aug1)\n","  * Random Horizontal Flip\n","  * Normalization\n","* Augmentation 2 (Aug2)\n","  * Random Horizontal Flip\n","  * Random Rotation (10 degrees)\n","  * Normalization\n","* Augmentation 3 (Aug3)\n","  * Random Horizontal Flip\n","  * Color Jitter (Brightness, Contrast, Saturation, Hue)\n","  * Normalization\n","* Augmentation 4 (Aug4)\n","  * Random Horizontal Flip\n","  * Random Rotation (10 degrees)\n","  * Color Jitter (Brightness, Contrast, Saturation, Hue)\n","  * Normalization\n","\n","**Results 1**:\n","* The models were trained for 30 epochs, and the best model  were recorded on the validation set whereas the Latest model is the last model saved(at the end of the training, 30 epoch). Below are the results for each augmentation strategy.\n","\n","  * **Augmentation 1[Validation Accuracy]**:\n","    * Best Model (Epoch 27):\n","      * Loss: 0.525\n","      * Top-1 Accuracy: 84.70%\n","      * Top-5 Accuracy: 99.40%\n","   * Latest Model (Epoch 30):\n","     * Loss: 0.65\n","     * Top-1 Accuracy: 83.08%\n","     * Top-5 Accuracy: 98.90%\n","  * **Augmentation 2[Validation Accuracy]**:\n","   * Best Model (Epoch 30):\n","     * Loss: 0.49\n","     * Top-1 Accuracy: 84.30%\n","     * Top-5 Accuracy: 99.06%\n","    * Latest Model (Epoch 30, Same as Best):\n","     * Loss: 0.49\n","     * Top-1 Accuracy: 84.30%\n","     * Top-5 Accuracy: 99.06%\n","\n","  * **Augmentation 3[Validation Accuracy]**:\n","    * Best Model (Epoch 26):\n","      * Loss: 0.524\n","      * Top-1 Accuracy: 84.02%\n","      * Top-5 Accuracy: 98.76%\n","    * Latest Model (Epoch 30):\n","      * Loss: 0.58\n","      * Top-1 Accuracy: 82.70%\n","      * Top-5 Accuracy: 98.84%\n","  * **Augmentation 4[Validation Accuracy]**:\n","    * Best Model (Epoch 27):\n","      * Loss: 0.4920\n","      * Top-1 Accuracy: 83.14%\n","      * Top-5 Accuracy: 99.20%\n","    * Latest Model (Epoch 30):\n","     * Loss: 0.558\n","     * Top-1 Accuracy: 81.60%\n","     * Top-5 Accuracy: 98.96%\n","\n","**Results 2**:\n","* These results were evaluated on the test set after the training of the model was complete.\n","\n","  * **Augmentation 1**:\n","    * Best Model (Epoch 27):\n","      * Loss: 0.5662\n","      * Top-1 Accuracy: 83.68%\n","      * Top-5 Accuracy: 99.25%\n","   * Latest Model (Epoch 30):\n","     * Loss: 0.6962\n","     * Top-1 Accuracy: 81.09%\n","     * Top-5 Accuracy: 98.88%\n","  * **Augmentation 2**:\n","   * Best Model (Epoch 30):\n","     * Loss: 0.4616\n","     * Top-1 Accuracy: 85.15%\n","     * Top-5 Accuracy: 99.39%\n","    * Latest Model (Epoch 30, Same as Best):\n","     * Loss: 0.4616\n","     * Top-1 Accuracy: 85.15%\n","     * Top-5 Accuracy: 99.39%\n","\n","  * **Augmentation 3**:\n","    * Best Model (Epoch 26):\n","      * Loss: 0.5484\n","      * Top-1 Accuracy: 83.01%\n","      * Top-5 Accuracy: 99.13%\n","    * Latest Model (Epoch 30):\n","      * Loss: 0.5747\n","      * Top-1 Accuracy: 82.89%\n","      * Top-5 Accuracy: 98.88%\n","  * **Augmentation 4**:\n","    * Best Model (Epoch 27):\n","      * Loss: 0.4600\n","      * Top-1 Accuracy: 84.60%\n","      * Top-5 Accuracy: 99.38%\n","    * Latest Model (Epoch 30):\n","      * Loss: 0.5084\n","      * Top-1 Accuracy: 83.43%\n","      * Top-5 Accuracy: 99.20%\n","\n","**Analysis**:\n","* The data augmentation strategies had a notable impact on the performance of the CNN. Augmentation 2, which included random horizontal flip and random rotation, resulted in the highest top-1 and top-5 accuracies on the test set, with the model performing consistently well up to the final epoch. Similarly, Augmentation 4, which combined horizontal flip, rotation, and color jitter, also achieved high performance.\n","\n","**Best Models vs. Latest Models**:\n","* For most augmentations, the best models were found before the final epoch, indicating that overfitting could have occurred if training continued without early stopping.\n","Augmentation 2 was unique in that the best model and the latest model were identical, suggesting consistent performance across epochs without significant overfitting.\n","\n","**Conclusion**:\n","* Data augmentation significantly enhanced the performance of the CNN on the CIFAR-10 dataset. Augmentations that combined multiple transformations, such as random rotation and color jitter, provided better results, reducing overfitting and improving generalization. These findings shows the importance of incorporating robust data augmentation strategies in training pipelines to achieve superior model performance.\n","<br>\n","<br>\n","<br>\n","\n","# **Report on Dropout in CNN Training on CIFAR-10 Dataset**\n","\n","## Introduction\n","\n","Dropout is a regularization technique used in neural networks to prevent overfitting by randomly dropping units during training. This report examines the impact of different dropout probabilities on the performance of a Convolutional Neural Network (CNN) trained on the CIFAR-10 dataset. We trained the model with dropout probabilities of 0.1, 0.3, 0.5, 0.7, and 0.9, and evaluated the performance of both the best and the latest models after running 30 epochs.\n","\n","## Results on the validation set\n","\n","### Dropout Probability: 0.1\n","\n","- **Best Model (Epoch 27):**\n","  - Loss: 0.46388\n","  - Top-1 Accuracy: 85.60%\n","  - Top-5 Accuracy: 99.42%\n","- **Latest Model (Epoch 30):**\n","  - Loss: 0.50982\n","  - Top-1 Accuracy: 84.52%\n","  - Top-5 Accuracy: 99.22%\n","\n","### Dropout Probability: 0.3\n","\n","- **Best Model (Epoch 30):**\n","  - Loss: 0.40893\n","  - Top-1 Accuracy: 86.22%\n","  - Top-5 Accuracy: 99.42%\n","- **Latest Model (Epoch 30):**\n","  - Same as Best Model\n","\n","### Dropout Probability: 0.5\n","\n","- **Best Model (Epoch 27):**\n","  - Loss: 0.49091\n","  - Top-1 Accuracy: 83.46%\n","  - Top-5 Accuracy: 99.22%\n","- **Latest Model (Epoch 30):**\n","  - Loss: 0.49747\n","  - Top-1 Accuracy: 83.30%\n","  - Top-5 Accuracy: 99.28%\n","\n","### Dropout Probability: 0.7\n","\n","- **Best Model (Epoch 29):**\n","  - Loss: 0.77345\n","  - Top-1 Accuracy: 72.94%\n","  - Top-5 Accuracy: 98.38%\n","- **Latest Model (Epoch 30):**\n","  - Loss: 0.86300\n","  - Top-1 Accuracy: 70.12%\n","  - Top-5 Accuracy: 98.06%\n","\n","### Dropout Probability: 0.9\n","\n","- **Best Model (Epoch 3):**\n","  - Loss: 3.11373\n","  - Top-1 Accuracy: 12.04%\n","  - Top-5 Accuracy: 54.18%\n","- **Latest Model (Epoch 30):**\n","  - Loss: 3.15423\n","  - Top-1 Accuracy: 9.98%\n","  - Top-5 Accuracy: 53.06%\n","\n","## Analysis\n","\n","The results indicate that the model's performance is relatively stable with dropout probabilities up to 0.5, with top-1 accuracy ranging from approximately 86% to 83%. However, there is a significant drop in accuracy when the dropout probability increases from 0.5 to 0.7, with top-1 accuracy falling to around 73%. Further increasing the dropout probability to 0.9 results in an exponential decline in accuracy, dropping to about 12%.\n","\n","### Performance Trend\n","\n","- **0.1 to 0.5 Dropout:** Marginal decrease in accuracy (86% to 83%)\n","- **0.5 to 0.7 Dropout:** Significant accuracy loss (83% to 73%)\n","- **0.7 to 0.9 Dropout:** Exponential accuracy drop (73% to 12%)\n","\n","## Conclusion\n","\n","Dropout is effective in preventing overfitting at lower probabilities (0.1 to 0.5), but higher probabilities (0.7 and above) severely degrade the model's performance. Optimal dropout probabilities for this CNN on the CIFAR-10 dataset appear to be within the range of 0.1 to 0.3, balancing regularization and maintaining high accuracy.\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AOgg_0Scc4uL"},"source":["### Question 4: Use pretrained networks (10 points)\n","\n","It has become standard practice in computer vision tasks related to images to use a convolutional network pre-trained as the backbone feature extraction network and train new layers on top for the target task. In this question, we will implement such a model. We will use the `VGG_11_bn` network from the `torchvision.models` library as our backbone network. This model has been trained on ImageNet, achieving a top-5 error rate of 10.19%. It consists of 8 convolutional layers followed by adaptive average pooling and fully-connected layers to perform the classification. We will get rid of the average pooling and fully-connected layers from the `VGG_11_bn` model and attach our own fully connected layers to perform the CIFAR-10 classification.\n","\n","a) Instantiate a pretrained version of the `VGG_11_bn` model with ImageNet pre-trained weights. Add two fully connected layers on top, with Batch Norm and ReLU layers in between them, to build the CIFAR-10 10-class classifier. Note that you will need to set the correct mean and variance in the data-loader, to match the mean and variance the data was normalized with when the `VGG_11_bn` was trained. Train only the newly added layers while disabling gradients for the rest of the network. Each parameter in PyTorch has a required grad flag, which can be turned off to disable gradient computation for it. Get familiar with this gradient control mechanism in PyTorch and train the above model. As a reference point, you will see validation accuracies in the range (61-65%) if implemented correctly. (6 points)\n","\n","b) We can see that while the ImageNet features are useful, just learning the new layers does not yield better performance than training our own network from scratch. This is due to the domain-shift between the ImageNet dataset (224x224 resolution images) and the CIFAR-10 dataset (32x32 images). To improve the performance we can fine-tune the whole network on the CIFAR-10 dataset, starting from the ImageNet initialization (set `\"fine_tune\"` to `true` in `vgg_cifar10.py`). To do this, enable gradient computation to the rest of the network, and update all the model parameters. Additionally train a baseline model where the entire network is trained from scratch, without loading the ImageNet weights (set `\"weights\"` to `None` in `vgg_cifar10.py`). Compare the two models' training curves, validation, and testing performance in the report. (4 points)\n","\n","\n","If you're using Pytorch 1, the `weights` argument will not work. In that case, you need to change the `weights` argument to `pretrained=True` or `False`. Feel free to post on Forum if you have any issues.\n","\n","For both questions, feel free to modify the data augmentation by defining a new preset and referring to it in the config file. However, make sure that in your experiments you always change only one thing at a time (i.e use the same augmentation for both method A and method B if you're comparing them with each other!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-0PN3WlNdBQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718089055839,"user_tz":420,"elapsed":23163,"user":{"displayName":"Siddharth Seth","userId":"17018082859390637229"}},"outputId":"da6a2679-d469-4112-a1c7-89c586c4e45c"},"outputs":[{"output_type":"stream","name":"stdout","text":["transforms for preset CIFAR10_VGG for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 64, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 64, 'shuffle': False, 'num_workers': 6}\n","transforms for preset CIFAR10_VGG for split eval are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Initialization DataLoader for 10000 samples with {'batch_size': 64, 'shuffle': False, 'num_workers': 6}\n"]}],"source":["import torch\n","from src.trainers.vgg_trainer import VGGTrainer\n","from copy import deepcopy\n","from src.data_loaders.data_modules import CIFAR10DataModule\n","from cfgs.exercise_3 import vgg_cifar10\n","q4_config = vgg_cifar10.q4_dict\n","\n","\n","datamodule_class = q4_config['datamodule']\n","data_args = q4_config['data_args']\n","\n","dm = datamodule_class(**data_args)\n","\n","# Based on the heldout_split in the config file,\n","# the datamodule will break the dataset into two splits\n","train_data_loader = dm.get_loader()\n","valid_data_loader = dm.get_heldout_loader()\n","\n","# Test loader is the same as train loader\n","# except that training=False, shuffle=False, and no splitting is done\n","# So we use the exact config from training and just modify these arguments\n","test_data_args = deepcopy(data_args) # copy the args\n","test_data_args['training']=False\n","test_data_args['shuffle']=False\n","test_data_args['heldout_split']=0.0\n","\n","# Now we initialize the test module with the modified config\n","test_dm = datamodule_class(**test_data_args)\n","test_loader = test_dm.get_loader() # and get the loader from it"]},{"cell_type":"markdown","metadata":{"id":"LxCQd-a5NdBQ"},"source":[" By default WandB is enabled in config file for `vgg_cifar10.py`. You can set it to false if you don't want to use it. It's not an essential part of the assignment anyway."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"knqAGDDANdBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718051382643,"user_tz":-120,"elapsed":9334,"user":{"displayName":"Parul Negi","userId":"14413694775804507286"}},"outputId":"84d4c8a0-2345-4343-e4ff-f6c7716e4739"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n","100%|██████████| 507M/507M [00:06<00:00, 82.3MB/s]\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"]}],"source":["trainer_class = q4_config['trainer_module']\n","trainer_vgg = trainer_class(\n","    config = q4_config,\n","    log_dir = ospj(PROJECT_ROOT,'Logs'),\n","    train_loader=train_data_loader,\n","    eval_loader=valid_data_loader,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7N5_sSvNdBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718051581908,"user_tz":-120,"elapsed":197243,"user":{"displayName":"Parul Negi","userId":"14413694775804507286"}},"outputId":"9f5e5cdc-0ef9-4e9c-e4b9-53758ca6ce8d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Train Epoch: 1 Loss: 1.2975: : 100% 45056/45056 [00:21<00:00, 2093.64it/s]\n","Eval Loss: 1.7378: : 100% 5056/5056 [00:02<00:00, 2342.77it/s]\n","Train Epoch: 2 Loss: 2.6422: : 100% 45056/45056 [00:23<00:00, 1922.69it/s]\n","Eval Loss: 1.8076: : 100% 5056/5056 [00:03<00:00, 1538.26it/s]\n","Train Epoch: 3 Loss: 1.6476: : 100% 45056/45056 [00:21<00:00, 2143.80it/s]\n","Eval Loss: 1.4517: : 100% 5056/5056 [00:02<00:00, 2374.96it/s]\n","Train Epoch: 4 Loss: 0.7677: : 100% 45056/45056 [00:21<00:00, 2112.03it/s]\n","Eval Loss: 1.6699: : 100% 5056/5056 [00:02<00:00, 2349.80it/s]\n","Train Epoch: 5 Loss: 0.6213: : 100% 45056/45056 [00:21<00:00, 2066.98it/s]\n","Eval Loss: 1.4723: : 100% 5056/5056 [00:03<00:00, 1594.24it/s]\n","Train Epoch: 6 Loss: 1.2235: : 100% 45056/45056 [00:21<00:00, 2089.97it/s]\n","Eval Loss: 1.3484: : 100% 5056/5056 [00:02<00:00, 2372.28it/s]\n","Train Epoch: 7 Loss: 1.6916: : 100% 45056/45056 [00:21<00:00, 2085.56it/s]\n","Eval Loss: 1.7556: : 100% 5056/5056 [00:02<00:00, 2328.29it/s]\n","Train Epoch: 8 Loss: 1.2140: : 100% 45056/45056 [00:22<00:00, 1975.88it/s]\n","Eval Loss: 1.3325: : 100% 5056/5056 [00:03<00:00, 1386.17it/s]\n"]}],"source":["trainer_vgg.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZfxv-_YNdBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718051827169,"user_tz":-120,"elapsed":9799,"user":{"displayName":"Parul Negi","userId":"14413694775804507286"}},"outputId":"4061e78d-c06f-460d-bba7-cb05b3e3d1a2"},"outputs":[{"output_type":"stream","name":"stderr","text":["Eval Loss: 1.1149: : 100% 10048/10048 [00:04<00:00, 2109.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Best model results: {'loss': 1.0156034519717951, 'top1': 0.643312101910828, 'top5': 0.9635748407643312}\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.8957: : 100% 10048/10048 [00:04<00:00, 2211.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Last model results: {'loss': 1.0578762281472516, 'top1': 0.6476910828025477, 'top5': 0.9643710191082803}\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Change this to the experiment you want to evaluate\n","# Save the best model path\n","best_model_path = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_VGG/best_val_model.pth'\n","last_model_path = '/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/CIFAR10_VGG/last_model.pth'\n","\n","# Load and evaluate the best model\n","trainer_vgg.load_model(path=best_model_path)\n","result_best = trainer_vgg.evaluate(loader=test_loader)\n","print(f\"Best model results: {result_best}\")\n","\n","# Load and evaluate the last model\n","trainer_vgg.load_model(path=last_model_path)\n","result_last = trainer_vgg.evaluate(loader=test_loader)\n","print(f\"Last model results: {result_last}\")\n","\n"]},{"cell_type":"code","source":["# b\n","from copy import deepcopy\n","import torch\n","from src.trainers.vgg_trainer import VGGTrainer\n","from copy import deepcopy\n","from src.data_loaders.data_modules import CIFAR10DataModule\n","from cfgs.exercise_3 import vgg_cifar10\n","\n","configs = [vgg_cifar10.q4_dictB, vgg_cifar10.q4_scratch]\n","results = {}\n","\n","for config in configs:\n","    experiment_name = config['name']\n","    print(f\"Running experiment: {experiment_name}\")\n","\n","    datamodule_class = config['datamodule']\n","    data_args = config['data_args']\n","\n","    dm = datamodule_class(**data_args)\n","\n","    train_data_loader = dm.get_loader()\n","    valid_data_loader = dm.get_heldout_loader()\n","\n","    test_data_args = deepcopy(data_args)\n","    test_data_args['training'] = False\n","    test_data_args['shuffle'] = False\n","    test_data_args['heldout_split'] = 0.0\n","\n","    test_dm = datamodule_class(**test_data_args)\n","    test_loader = test_dm.get_loader()\n","\n","    trainer_class = config['trainer_module']\n","    trainer = trainer_class(\n","        config=config,\n","        log_dir = ospj(PROJECT_ROOT,'Logs'),\n","        train_loader=train_data_loader,\n","        eval_loader=valid_data_loader,\n","    )\n","\n","    trainer.train()\n","\n","    best_model_path = f'/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/{experiment_name}/best_val_model.pth'\n","    last_model_path = f'/content/drive/MyDrive/Colab Notebooks/HLCV Assignment_3/Saved/{experiment_name}/last_model.pth'\n","\n","\n","    trainer.load_model(path=best_model_path)\n","    result_best = trainer.evaluate(loader=test_loader)\n","    print(f\"Best model results for {experiment_name}: {result_best}\")\n","\n","    trainer.load_model(path=last_model_path)\n","    result_last = trainer.evaluate(loader=test_loader)\n","    print(f\"Last model results for {experiment_name}: {result_last}\")\n","\n","    results[experiment_name] = {\n","        'best': result_best,\n","        'last': result_last\n","    }\n","# # Display results\n","# for experiment_name, result in results.items():\n","#     print(f\"Results for {experiment_name}:\")\n","#     print(f\"Best model accuracy: {result['best']['eval_top1']}\")\n","#     print(f\"Last model accuracy: {result['last']['eval_top1']}\")"],"metadata":{"id":"_NAJ4BcvgODp","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1718206494741,"user_tz":-120,"elapsed":2940953,"user":{"displayName":"Parul Negi","userId":"14413694775804507286"}},"outputId":"4ccb9ceb-4da3-4603-9843-12017d1900bd","collapsed":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Running experiment: CIFAR10_VGG_B\n","transforms for preset CIFAR10_VGG for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 64, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 64, 'shuffle': False, 'num_workers': 6}\n","transforms for preset CIFAR10_VGG for split eval are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Initialization DataLoader for 10000 samples with {'batch_size': 64, 'shuffle': False, 'num_workers': 6}\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n","100%|██████████| 507M/507M [00:05<00:00, 89.9MB/s]\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n","Train Epoch: 1 Loss: 1.6192: : 100% 45056/45056 [00:55<00:00, 809.72it/s]\n","Eval Loss: 1.4299: : 100% 5056/5056 [00:03<00:00, 1598.23it/s]\n","Train Epoch: 2 Loss: 0.6787: : 100% 45056/45056 [00:53<00:00, 849.54it/s]\n","Eval Loss: 0.7568: : 100% 5056/5056 [00:02<00:00, 2208.93it/s]\n","Train Epoch: 3 Loss: 1.6725: : 100% 45056/45056 [00:53<00:00, 843.32it/s]\n","Eval Loss: 0.7505: : 100% 5056/5056 [00:02<00:00, 2255.96it/s]\n","Train Epoch: 4 Loss: 0.8390: : 100% 45056/45056 [00:53<00:00, 838.61it/s]\n","Eval Loss: 1.0054: : 100% 5056/5056 [00:02<00:00, 2285.46it/s]\n","Train Epoch: 5 Loss: 0.0879: : 100% 45056/45056 [00:52<00:00, 855.23it/s]\n","Eval Loss: 0.9119: : 100% 5056/5056 [00:03<00:00, 1427.89it/s]\n","Train Epoch: 6 Loss: 1.7531: : 100% 45056/45056 [00:53<00:00, 836.03it/s]\n","Eval Loss: 1.3542: : 100% 5056/5056 [00:02<00:00, 2141.56it/s]\n","Train Epoch: 7 Loss: 0.0488: : 100% 45056/45056 [00:53<00:00, 838.98it/s]\n","Eval Loss: 0.8149: : 100% 5056/5056 [00:02<00:00, 2254.81it/s]\n","Train Epoch: 8 Loss: 0.0052: : 100% 45056/45056 [00:53<00:00, 837.92it/s]\n","Eval Loss: 1.6988: : 100% 5056/5056 [00:02<00:00, 2064.90it/s]\n","Train Epoch: 9 Loss: 0.1145: : 100% 45056/45056 [00:53<00:00, 839.95it/s]\n","Eval Loss: 1.5416: : 100% 5056/5056 [00:03<00:00, 1571.57it/s]\n","Train Epoch: 10 Loss: 2.3141: : 100% 45056/45056 [00:52<00:00, 852.02it/s]\n","Eval Loss: 1.4771: : 100% 5056/5056 [00:02<00:00, 2177.98it/s]\n","Train Epoch: 11 Loss: 0.8050: : 100% 45056/45056 [00:53<00:00, 843.11it/s]\n","Eval Loss: 0.9374: : 100% 5056/5056 [00:02<00:00, 2236.06it/s]\n","Train Epoch: 12 Loss: 0.0585: : 100% 45056/45056 [00:53<00:00, 847.97it/s]\n","Eval Loss: 1.4813: : 100% 5056/5056 [00:03<00:00, 1551.44it/s]\n","Train Epoch: 13 Loss: 0.0134: : 100% 45056/45056 [00:52<00:00, 850.51it/s]\n","Eval Loss: 1.3850: : 100% 5056/5056 [00:02<00:00, 2259.31it/s]\n","Train Epoch: 14 Loss: 1.0912: : 100% 45056/45056 [00:53<00:00, 844.69it/s]\n","Eval Loss: 0.6440: : 100% 5056/5056 [00:02<00:00, 2173.05it/s]\n","Train Epoch: 15 Loss: 0.0006: : 100% 45056/45056 [00:52<00:00, 855.63it/s]\n","Eval Loss: 0.9289: : 100% 5056/5056 [00:02<00:00, 1724.72it/s]\n","Train Epoch: 16 Loss: 0.5039: : 100% 45056/45056 [00:54<00:00, 833.40it/s]\n","Eval Loss: 1.1979: : 100% 5056/5056 [00:02<00:00, 2169.28it/s]\n","Train Epoch: 17 Loss: 0.0062: : 100% 45056/45056 [00:53<00:00, 845.05it/s]\n","Eval Loss: 0.8790: : 100% 5056/5056 [00:02<00:00, 2188.68it/s]\n","Train Epoch: 18 Loss: 0.5937: : 100% 45056/45056 [00:52<00:00, 854.63it/s]\n","Eval Loss: 0.9419: : 100% 5056/5056 [00:03<00:00, 1611.28it/s]\n","Train Epoch: 19 Loss: 0.0086: : 100% 45056/45056 [00:53<00:00, 845.76it/s]\n","Eval Loss: 0.6855: : 100% 5056/5056 [00:02<00:00, 2180.40it/s]\n","Train Epoch: 20 Loss: 0.7435: : 100% 45056/45056 [00:53<00:00, 841.04it/s]\n","Eval Loss: 0.7156: : 100% 5056/5056 [00:02<00:00, 2163.44it/s]\n","Train Epoch: 21 Loss: 0.2149: : 100% 45056/45056 [00:52<00:00, 851.24it/s]\n","Eval Loss: 1.4290: : 100% 5056/5056 [00:03<00:00, 1497.41it/s]\n","Train Epoch: 22 Loss: 0.0599: : 100% 45056/45056 [00:53<00:00, 846.01it/s]\n","Eval Loss: 0.7874: : 100% 5056/5056 [00:02<00:00, 2172.36it/s]\n","Train Epoch: 23 Loss: 0.0068: : 100% 45056/45056 [00:53<00:00, 846.22it/s]\n","Eval Loss: 1.6007: : 100% 5056/5056 [00:02<00:00, 2202.40it/s]\n","Train Epoch: 24 Loss: 0.2826: : 100% 45056/45056 [00:52<00:00, 853.37it/s]\n","Eval Loss: 2.4218: : 100% 5056/5056 [00:03<00:00, 1567.91it/s]\n","Train Epoch: 25 Loss: 0.0005: : 100% 45056/45056 [00:52<00:00, 853.51it/s]\n","Eval Loss: 0.7568: : 100% 5056/5056 [00:02<00:00, 2231.42it/s]\n","Eval Loss: 0.4442: : 100% 10048/10048 [00:04<00:00, 2367.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Best model results for CIFAR10_VGG_B: {'loss': 0.6256569487748632, 'top1': 0.8474323248407644, 'top5': 0.9914410828025477}\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.2914: : 100% 10048/10048 [00:04<00:00, 2347.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Last model results for CIFAR10_VGG_B: {'loss': 0.6822656414880874, 'top1': 0.8542993630573248, 'top5': 0.9892515923566879}\n","Running experiment: CIFAR10_VGG_Scratch\n","transforms for preset CIFAR10_VGG for split train are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 45000 samples with {'batch_size': 64, 'shuffle': True, 'num_workers': 6}\n","Initialization heldout DataLoader 5000 samples with {'batch_size': 64, 'shuffle': False, 'num_workers': 6}\n","transforms for preset CIFAR10_VGG for split eval are Compose(\n","    ToTensor()\n","    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",")\n","Files already downloaded and verified\n","Initialization DataLoader for 10000 samples with {'batch_size': 64, 'shuffle': False, 'num_workers': 6}\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: 1 Loss: 0.9752: : 100% 45056/45056 [00:53<00:00, 837.37it/s]\n","Eval Loss: 1.8663: : 100% 5056/5056 [00:03<00:00, 1519.21it/s]\n","Train Epoch: 2 Loss: 0.7099: : 100% 45056/45056 [00:53<00:00, 836.66it/s]\n","Eval Loss: 1.7400: : 100% 5056/5056 [00:02<00:00, 1831.94it/s]\n","Train Epoch: 3 Loss: 0.4434: : 100% 45056/45056 [00:53<00:00, 838.36it/s]\n","Eval Loss: 1.5114: : 100% 5056/5056 [00:02<00:00, 2271.46it/s]\n","Train Epoch: 4 Loss: 0.4401: : 100% 45056/45056 [00:54<00:00, 827.41it/s]\n","Eval Loss: 2.2636: : 100% 5056/5056 [00:02<00:00, 2113.62it/s]\n","Train Epoch: 5 Loss: 1.2425: : 100% 45056/45056 [00:54<00:00, 830.18it/s]\n","Eval Loss: 2.1769: : 100% 5056/5056 [00:02<00:00, 2057.97it/s]\n","Train Epoch: 6 Loss: 0.9041: : 100% 45056/45056 [00:53<00:00, 835.38it/s]\n","Eval Loss: 0.8067: : 100% 5056/5056 [00:03<00:00, 1545.85it/s]\n","Train Epoch: 7 Loss: 1.4790: : 100% 45056/45056 [00:53<00:00, 846.24it/s]\n","Eval Loss: 1.9401: : 100% 5056/5056 [00:02<00:00, 2207.14it/s]\n","Train Epoch: 8 Loss: 0.6701: : 100% 45056/45056 [00:53<00:00, 838.43it/s]\n","Eval Loss: 2.1589: : 100% 5056/5056 [00:02<00:00, 2200.68it/s]\n","Train Epoch: 9 Loss: 0.1344: : 100% 45056/45056 [00:53<00:00, 841.49it/s]\n","Eval Loss: 1.2609: : 100% 5056/5056 [00:02<00:00, 2166.49it/s]\n","Train Epoch: 10 Loss: 0.0072: : 100% 45056/45056 [00:53<00:00, 843.10it/s]\n","Eval Loss: 1.0108: : 100% 5056/5056 [00:02<00:00, 2102.50it/s]\n","Train Epoch: 11 Loss: 0.6016: : 100% 45056/45056 [00:53<00:00, 848.46it/s]\n","Eval Loss: 1.0686: : 100% 5056/5056 [00:03<00:00, 1652.95it/s]\n","Train Epoch: 12 Loss: 0.0041: : 100% 45056/45056 [00:53<00:00, 841.87it/s]\n","Eval Loss: 2.4887: : 100% 5056/5056 [00:02<00:00, 2138.87it/s]\n","Train Epoch: 13 Loss: 0.0400: : 100% 45056/45056 [00:53<00:00, 844.15it/s]\n","Eval Loss: 2.4185: : 100% 5056/5056 [00:02<00:00, 2130.33it/s]\n","Train Epoch: 14 Loss: 0.0216: : 100% 45056/45056 [00:53<00:00, 834.63it/s]\n","Eval Loss: 1.9562: : 100% 5056/5056 [00:02<00:00, 2065.64it/s]\n","Train Epoch: 15 Loss: 0.0735: : 100% 45056/45056 [00:53<00:00, 843.33it/s]\n","Eval Loss: 2.3361: : 100% 5056/5056 [00:03<00:00, 1604.20it/s]\n","Train Epoch: 16 Loss: 0.3490: : 100% 45056/45056 [00:55<00:00, 805.47it/s]\n","Eval Loss: 3.0264: : 100% 5056/5056 [00:03<00:00, 1462.07it/s]\n","Train Epoch: 17 Loss: 0.0069: : 100% 45056/45056 [00:54<00:00, 830.10it/s]\n","Eval Loss: 1.6074: : 100% 5056/5056 [00:03<00:00, 1507.33it/s]\n","Train Epoch: 18 Loss: 0.1696: : 100% 45056/45056 [00:53<00:00, 839.89it/s]\n","Eval Loss: 2.4732: : 100% 5056/5056 [00:02<00:00, 2146.19it/s]\n","Train Epoch: 19 Loss: 0.0529: : 100% 45056/45056 [00:54<00:00, 829.66it/s]\n","Eval Loss: 2.3993: : 100% 5056/5056 [00:02<00:00, 2134.20it/s]\n","Train Epoch: 20 Loss: 0.0014: : 100% 45056/45056 [00:54<00:00, 831.15it/s]\n","Eval Loss: 1.4287: : 100% 5056/5056 [00:02<00:00, 2029.41it/s]\n","Train Epoch: 21 Loss: 0.0043: : 100% 45056/45056 [00:53<00:00, 835.20it/s]\n","Eval Loss: 1.8801: : 100% 5056/5056 [00:03<00:00, 1436.21it/s]\n","Train Epoch: 22 Loss: 0.0277: : 100% 45056/45056 [00:53<00:00, 844.44it/s]\n","Eval Loss: 1.6692: : 100% 5056/5056 [00:02<00:00, 2023.46it/s]\n","Train Epoch: 23 Loss: 0.0009: : 100% 45056/45056 [00:53<00:00, 836.29it/s]\n","Eval Loss: 1.7656: : 100% 5056/5056 [00:02<00:00, 2120.35it/s]\n","Train Epoch: 24 Loss: 0.5988: : 100% 45056/45056 [00:53<00:00, 845.33it/s]\n","Eval Loss: 1.6512: : 100% 5056/5056 [00:02<00:00, 1719.49it/s]\n","Train Epoch: 25 Loss: 0.0063: : 100% 45056/45056 [00:53<00:00, 840.68it/s]\n","Eval Loss: 2.3037: : 100% 5056/5056 [00:02<00:00, 2093.37it/s]\n","Eval Loss: 0.8390: : 100% 10048/10048 [00:05<00:00, 1834.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Best model results for CIFAR10_VGG_Scratch: {'loss': 0.8285959376271363, 'top1': 0.8278264331210191, 'top5': 0.9821855095541401}\n"]},{"output_type":"stream","name":"stderr","text":["Eval Loss: 0.9582: : 100% 10048/10048 [00:05<00:00, 1816.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Last model results for CIFAR10_VGG_Scratch: {'loss': 0.8502242458853752, 'top1': 0.8246417197452229, 'top5': 0.9856687898089171}\n","Results for CIFAR10_VGG_B:\n"]},{"output_type":"error","ename":"KeyError","evalue":"'eval_top1'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c42523dcdcab>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Results for {experiment_name}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best model accuracy: {result['best']['eval_top1']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Last model accuracy: {result['last']['eval_top1']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'eval_top1'"]}]},{"cell_type":"code","source":["# Display results\n","for experiment_name, result in results.items():\n","    print(f\"Results for {experiment_name}:\")\n","    print(f\"Last model accuracy: {result}\")"],"metadata":{"id":"au_44e5aNXsR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Results for CIFAR10_VGG_B:\n","Best model accuracy: {'best': {'loss': 0.6256569487748632, 'top1': 0.8474323248407644, 'top5': 0.9914410828025477}, 'last': {'loss': 0.6822656414880874, 'top1': 0.8542993630573248, 'top5': 0.9892515923566879}}\n","\n","Results for CIFAR10_VGG_Scratch:\n","Best model accuracy: {'best': {'loss': 0.8285959376271363, 'top1': 0.8278264331210191, 'top5': 0.9821855095541401}, 'last': {'loss': 0.8502242458853752, 'top1': 0.8246417197452229, 'top5': 0.9856687898089171}}"],"metadata":{"id":"1cE2XaiCNx9Z"}},{"cell_type":"markdown","metadata":{"id":"-Uvjz11dc4uN"},"source":["# Report on Training a Pretrained VGG-11_bn Network for CIFAR-10 Classification\n","\n","## Introduction\n","\n","In this task, we use a pretrained VGG-11_bn model with ImageNet weights and adapt it to classify CIFAR-10 images into 10 classes. We add two fully connected layers on top of the pretrained network, including Batch Normalization and ReLU activation functions between them. The newly added layers are trained while the rest of the network's parameters are frozen, utilizing gradient control in PyTorch. The model is expected to achieve validation accuracies in the range of 61-65% if implemented correctly.\n","\n","## Implementation Details\n","\n","1. **Model Architecture:**\n","   - Base Model: VGG-11_bn pretrained on ImageNet.\n","   - Additional Layers:\n","     - Fully Connected Layer\n","     - Batch Normalization\n","     - ReLU Activation\n","     - Fully Connected Layer (output layer for 10 classes)\n","\n","2. **Training Strategy:**\n","   - Train only the newly added layers by disabling gradients for the rest of the network.\n","   - Early stopping steps 5 was employed, resulting in training stopping at epoch 8 out of 25 epochs.\n","\n","3. **Saving Models:**\n","   - The best model based on validation accuracy was saved.\n","   - The latest model at the end of training was also saved.\n","\n","## Training and Evaluation Results\n","\n","### Best Model (Epoch 3)\n","\n","- **Training Metrics:**\n","  - Loss: 0.98835\n","  - Top-1 Accuracy: 65.11%\n","  - Top-5 Accuracy: 96.96%\n","- **Validation Metrics:**\n","  - Loss: 1.00970\n","  - Top-1 Accuracy: 65.03%\n","  - Top-5 Accuracy: 96.30%\n","\n","### Latest Model (Epoch 7)\n","\n","- **Training Metrics:**\n","  - Loss: 0.77684\n","  - Top-1 Accuracy: 72.41%\n","  - Top-5 Accuracy: 98.39%\n","- **Validation Metrics:**\n","  - Loss: 1.05102\n","  - Top-1 Accuracy: 65.03%\n","  - Top-5 Accuracy: 96.26%\n","\n","### Test Set Evaluation\n","\n","- **Best Model Results:**\n","  - Loss: 1.0156\n","  - Top-1 Accuracy: 64.33%\n","  - Top-5 Accuracy: 96.36%\n","- **Latest Model Results:**\n","  - Loss: 1.0579\n","  - Top-1 Accuracy: 64.77%\n","  - Top-5 Accuracy: 96.44%\n","\n","## Analysis\n","\n","The model achieved validation accuracies within the expected range of 61-65%. The best performance was observed early in training, with the highest validation accuracy achieved at epoch 3. The training process indicates that early stopping helped prevent overfitting, maintaining a balance between training and validation performance.\n","\n","### Performance Summary\n","\n","- **Validation Accuracy Range:** 61-65%\n","- **Best Epoch:** 3\n","- **Top-1 Accuracy (Best Model):** 65.03%\n","- **Top-1 Accuracy (Latest Model):** 65.03%\n","- **Test Accuracy (Best Model):** 64.33%\n","- **Test Accuracy (Latest Model):** 64.77%\n","\n","## Conclusion\n","\n","The pretrained VGG-11_bn model, fine-tuned with additional layers for CIFAR-10 classification, demonstrated low performance with validation accuracies reaching only up to 65.03%. Early stopping effectively managed training duration and helped in maintaining generalization. The results from using the pretrained model is not that impressive however, this approach confirms the utility of pretrained models in transfer learning scenarios for new classification tasks.\n","\n","\n","<br>\n","<br>\n","<br>\n","\n","# Report on Fine-Tuning vs. Training from Scratch for CIFAR-10 Classification using VGG-11_bn\n","\n","## Introduction\n","\n","This report presents the results of two approaches to training a VGG-11_bn model for CIFAR-10 classification:\n","1. Fine-tuning a pretrained VGG-11_bn model initialized with ImageNet weights.\n","2. Training a VGG-11_bn model from scratch without any pretrained weights.\n","\n","The aim is to compare the performance of these two methods in terms of training curves, validation, and testing performance.\n","\n","## Experiment Setup\n","\n","### Fine-Tuning Configuration\n","\n","For the fine-tuning task, we initialized the VGG-11_bn model with ImageNet weights and enabled gradient computation for all layers to update all model parameters.\n","\n","### Training from Scratch Configuration\n","\n","For training from scratch, we initialized the VGG-11_bn model without any pretrained weights and trained the entire network on CIFAR-10.\n","\n","### Training Details\n","\n","Both models were trained for 25 epochs without early stopping. The best model based on validation accuracy and the latest model at the end of training were saved for evaluation.\n","\n","## Results\n","\n","### Fine-Tune Model\n","\n","**Best Model (Epoch 8)**\n","- **Training Metrics:**\n","  - Loss: 0.10646\n","  - Top-1 Accuracy: 96.39%\n","  - Top-5 Accuracy: 99.98%\n","- **Validation Metrics:**\n","  - Loss: 0.57385\n","  - Top-1 Accuracy: 86.04%\n","  - Top-5 Accuracy: 99.31%\n","\n","**Latest Model (Epoch 25)**\n","- **Training Metrics:**\n","  - Loss: 0.03401\n","  - Top-1 Accuracy: 98.90%\n","  - Top-5 Accuracy: 99.99%\n","- **Validation Metrics:**\n","  - Loss: 0.65875\n","  - Top-1 Accuracy: 85.74%\n","  - Top-5 Accuracy: 98.93%\n","\n","**Test Set Evaluation**\n","- **Best Model Results:**\n","  - Loss: 0.62566\n","  - Top-1 Accuracy: 84.74%\n","  - Top-5 Accuracy: 99.14%\n","- **Latest Model Results:**\n","  - Loss: 0.68227\n","  - Top-1 Accuracy: 85.43%\n","  - Top-5 Accuracy: 98.93%\n","\n","### Model Trained from Scratch\n","\n","**Best Model (Epoch 20)**\n","- **Training Metrics:**\n","  - Loss: 0.04310\n","  - Top-1 Accuracy: 98.61%\n","  - Top-5 Accuracy: 99.99%\n","- **Validation Metrics:**\n","  - Loss: 0.77524\n","  - Top-1 Accuracy: 84.24%\n","  - Top-5 Accuracy: 98.30%\n","\n","**Latest Model (Epoch 25)**\n","- **Training Metrics:**\n","  - Loss: 0.04177\n","  - Top-1 Accuracy: 98.58%\n","  - Top-5 Accuracy: 100.00%\n","- **Validation Metrics:**\n","  - Loss: 0.83449\n","  - Top-1 Accuracy: 83.48%\n","  - Top-5 Accuracy: 98.62%\n","\n","**Test Set Evaluation**\n","- **Best Model Results:**\n","  - Loss: 0.82860\n","  - Top-1 Accuracy: 82.78%\n","  - Top-5 Accuracy: 98.22%\n","- **Latest Model Results:**\n","  - Loss: 0.85022\n","  - Top-1 Accuracy: 82.46%\n","  - Top-5 Accuracy: 98.57%\n","\n","## Analysis\n","\n","### Comparison of Performance\n","\n","- **Fine-Tuning vs. Training from Scratch:**\n","  - The fine-tuned model achieved higher validation and test accuracies compared to the model trained from scratch. This indicates that leveraging pretrained weights from ImageNet provides a significant performance boost, likely due to the transfer of learned features that are useful for image classification tasks.\n","  - The best model from fine-tuning reached a top-1 accuracy of 84.74% on the test set, whereas the best model trained from scratch achieved 82.78%.\n","  - Training loss and accuracy metrics indicate faster convergence and better generalization for the fine-tuned model.\n","\n","### Conclusion\n","\n","Fine-tuning a pretrained VGG-11_bn model initialized with ImageNet weights outperforms training the same model from scratch for CIFAR-10 classification. The domain shift between ImageNet and CIFAR-10 does not significantly hinder the performance when the entire network is fine-tuned. This demonstrates the advantage of using transfer learning and fine-tuning strategies in deep learning tasks involving different but related datasets.\n","<br>\n","<br>\n","<br>"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}